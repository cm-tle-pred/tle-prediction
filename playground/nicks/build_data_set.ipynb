{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sgp4.api import Satrec, WGS72\n",
    "from sgp4.conveniences import jday_datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the norads used in training\n",
    "train_norad_df = pd.read_pickle('train_norads.pkl.gz')\n",
    "train_norad_list = train_norad_df.norad.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#csv_store_path = os.environ['GP_HIST_PATH']\n",
    "csv_store_path = os.environ['my_home_path'] + '\\data\\space-track-gp-hist-sample'\n",
    "\n",
    "dfs = None\n",
    "files = sorted([x for x in os.listdir(f'{csv_store_path}/') if x.endswith(\".csv.gz\")])\n",
    "for f in tqdm(files):\n",
    "    #df = pd.read_csv(f'{csv_store_path}/{f}', parse_dates=['EPOCH'], infer_datetime_format=True, index_col='EPOCH', compression='gzip')\n",
    "    df = pd.read_csv(f'{csv_store_path}/{f}', parse_dates=['EPOCH'], infer_datetime_format=True, compression='gzip')\n",
    "    # LEO = Mean Motion > 11.25 and Eccentricity < 0.25\n",
    "    #df = df[(df.MEAN_MOTION > 11.25) & (df.ECCENTRICITY < 0.25)]\n",
    "    df = df[df.NORAD_CAT_ID.isin(train_norad_list)]\n",
    "\n",
    "    # Since animated gabbard diagrams are generated per frame, we can revert the scaling when we plot the graphs\n",
    "    if dfs is None:\n",
    "        dfs = df\n",
    "    else:\n",
    "        dfs = pd.concat([dfs,df])\n",
    "            \n",
    "# Remove unnecessary columns to save memory\n",
    "unnecessary_columns = ['CCSDS_OMM_VERS', 'COMMENT', 'CREATION_DATE', 'ORIGINATOR', 'OBJECT_NAME', 'OBJECT_ID',\n",
    "                       'CENTER_NAME', 'REF_FRAME', 'TIME_SYSTEM', 'MEAN_ELEMENT_THEORY', 'EPHEMERIS_TYPE',\n",
    "                       'CLASSIFICATION_TYPE', 'ELEMENT_SET_NO', 'REV_AT_EPOCH', 'SEMIMAJOR_AXIS', 'PERIOD',\n",
    "                       'APOAPSIS', 'PERIAPSIS', 'OBJECT_TYPE', 'RCS_SIZE', 'COUNTRY_CODE', 'LAUNCH_DATE',\n",
    "                       'SITE', 'DECAY_DATE', 'FILE', 'GP_ID', 'TLE_LINE0', 'index']\n",
    "dfs = dfs.reset_index().drop(columns=unnecessary_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satellite_position_data():\n",
    "    # Create the satellite object (used to find satellite position)\n",
    "    dfs['satobj'] = dfs.apply(lambda x: Satrec.twoline2rv(x['TLE_LINE1'], x['TLE_LINE2']), axis=1)\n",
    "\n",
    "    # Get the Julian date of the EPOCH\n",
    "    #dfs['epoch_julian'] = dfs['EPOCH'].apply(lambda x: jday_datetime(x.replace(tzinfo=timezone.utc)))\n",
    "    dfs[['epoch_jd', 'epoch_fr']] = dfs['EPOCH'].apply(lambda x: jday_datetime(x.replace(tzinfo=timezone.utc))).to_list()\n",
    "\n",
    "    # Get the days since 1949 December 31 00:00 UT\n",
    "    # This will be used when creating satobj for the test set\n",
    "    # (this is needed to get the satellite position from generated TLEs\n",
    "    #  because of how Satrec sgp4init() works')\n",
    "    ref_date = datetime.strptime('12/31/1949 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "    dfs['epoch_days'] = dfs['EPOCH'].apply(lambda x: (x-ref_date)/np.timedelta64(1, 'D'))\n",
    "\n",
    "    # Get satellite x,y,z positions from TLE\n",
    "    #dfs['satpos'] = dfs.apply(lambda x: x['satobj'].sgp4(*x['epoch_julian'])[1], axis=1)\n",
    "    dfs['satpos'] = dfs.apply(lambda x: np.array(x['satobj'].sgp4(x['epoch_jd'], x['epoch_fr'])[1]), axis=1)\n",
    "\n",
    "get_satellite_position_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xy():\n",
    "    \n",
    "    # ML Structure\n",
    "    # Input:\n",
    "    #  - Reference TLE Data (+ EPOCH)\n",
    "    #  - Target EPOCH\n",
    "    # Output:\n",
    "    #  - Target TLE Data\n",
    "    \n",
    "    def groups(lst):\n",
    "        arr = lst.copy()\n",
    "        np.random.shuffle(arr)\n",
    "        i=1\n",
    "        if len(lst)<=1:\n",
    "            return\n",
    "        while True:\n",
    "            if i==len(lst):\n",
    "                yield tuple((arr[i-1],arr[0]))\n",
    "                break\n",
    "            else:\n",
    "                yield tuple((arr[i-1],arr[i]))\n",
    "                i+=1\n",
    "    \n",
    "    # For each unique NORAD, find all TLE indexes and generate\n",
    "    # a list of combinations\n",
    "    idx_pairs = []\n",
    "    for norad in dfs['NORAD_CAT_ID'].unique():\n",
    "        norad_idxs = dfs[dfs['NORAD_CAT_ID']==norad].index.values\n",
    "        if len(norad_idxs > 1):\n",
    "            idx_pairs.extend(groups(norad_idxs))\n",
    "    idx_pairs = np.array(idx_pairs)\n",
    "    \n",
    "    # Build our X/Y datasets\n",
    "    X_all = dfs.loc[idx_pairs[:,0]].reset_index()\n",
    "    Y_all = dfs.loc[idx_pairs[:,1]].reset_index()\n",
    "    \n",
    "    # This will be the column that links x and y\n",
    "    key_columns = ['epoch_jd', 'epoch_fr']\n",
    "    target_columns = ['target_epoch_jd', 'target_epoch_fr']\n",
    "    X_all[target_columns] = Y_all[key_columns]\n",
    "    \n",
    "    return X_all, Y_all\n",
    "\n",
    "X_all, y_all = create_xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_X(X):\n",
    "    # Perform any cleaning of values\n",
    "    \n",
    "    # Return only necessary columns\n",
    "    X_columns = ['MEAN_MOTION_DOT', 'MEAN_MOTION_DDOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE',\n",
    "                 'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY', 'MEAN_MOTION',\n",
    "                 'epoch_jd', 'epoch_fr', 'target_epoch_jd', 'target_epoch_fr']\n",
    "    \n",
    "    return X[X_columns]\n",
    "    \n",
    "X_all_clean = clean_X(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Y(Y):\n",
    "    # Perform any cleaning of values\n",
    "    \n",
    "    # Return only necessary columns\n",
    "    Y_columns = ['MEAN_MOTION_DOT', 'MEAN_MOTION_DDOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE',\n",
    "                 'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY', 'MEAN_MOTION',\n",
    "                 'epoch_days', 'epoch_jd', 'epoch_fr', 'satpos']\n",
    "    \n",
    "    return Y[Y_columns]\n",
    "    \n",
    "y_all_clean = clean_Y(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_clean, y_all_clean, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "# Remove non-training columns from y_train\n",
    "non_training_cols = ['epoch_days', 'epoch_jd', 'epoch_fr', 'satpos']\n",
    "y_train = y_train.drop(columns=non_training_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra trees got score 0.516434536110915\n",
      "Random Forest got score 0.5414598281573768\n",
      "K-nn got score 0.37590439661058755\n",
      "Linear regression got score 0.419811728932068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "y_test = y_test.drop(columns=non_training_cols, axis=1)\n",
    "\n",
    "ESTIMATORS = {\n",
    "    \"Extra trees\": ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=10, random_state=0, n_jobs=-1),\n",
    "    #\"GradientBoost\": MultiOutputRegressor(GradientBoostingRegressor(n_estimators=10, random_state=0)),\n",
    "    \"K-nn\": KNeighborsRegressor(n_jobs=-1),\n",
    "    \"Linear regression\": LinearRegression(n_jobs=-1),\n",
    "    #\"Ridge\": RidgeCV(),\n",
    "    #\"SVM\": MultiOutputRegressor(SVR()),\n",
    "}\n",
    "\n",
    "# Let's see how it does on the same NORAD\n",
    "for name, estimator in ESTIMATORS.items():\n",
    "    estimator.fit(X_train, y_train)\n",
    "    score = estimator.score(X_test, y_test)\n",
    "    print (f'{name} got score {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterator.combinations()\n",
    "```\n",
    "Extra trees got score 0.9080013082091861\n",
    "Random Forest got score 0.883921000656464\n",
    "GradientBoost got score 0.5516253957176365\n",
    "K-nn got score 0.7644015286756071\n",
    "Linear regression got score 0.4746575589454271\n",
    "Ridge got score 0.4744900285859706\n",
    "```\n",
    "groups()\n",
    "```\n",
    "Extra trees got score 0.534262241603748\n",
    "Random Forest got score 0.4692323383272387\n",
    "GradientBoost got score 0.356691868881702\n",
    "K-nn got score 0.36862432289895664\n",
    "Linear regression got score 0.4765033608802853\n",
    "Ridge got score 0.45090731103313353\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.7557"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.loc[0,'RA_OF_ASC_NODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#model = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1).fit(X_train, y_train)\n",
    "model = LinearRegression(n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_error(results):\n",
    "    '''\n",
    "    This returns the Mean-Squared Error\n",
    "    of the predicted TLE's satellite position\n",
    "    vs the actional TLE's satellites position\n",
    "    '''\n",
    "    \n",
    "    def get_mserror(x):\n",
    "        return ((x['satpos']-x['satpos_calc'])**2).mean()\n",
    "\n",
    "    def get_satpos(x):\n",
    "        sat = Satrec()\n",
    "        sat.sgp4init(\n",
    "             WGS72,           # gravity model\n",
    "             'i',             # 'a' = old AFSPC mode, 'i' = improved mode\n",
    "             0,               # satnum: Satellite number\n",
    "             x['epoch_days'],       # epoch: days since 1949 December 31 00:00 UT\n",
    "             x['BSTAR'],      # bstar: d`rag coefficient (/earth radii)\n",
    "             x['MEAN_MOTION_DOT'], # ndot (NOT USED): ballistic coefficient (revs/day)\n",
    "             x['MEAN_MOTION_DDOT'],             # nddot (NOT USED): mean motion 2nd derivative (revs/day^3)\n",
    "             x['ECCENTRICITY'],       # ecco: eccentricity\n",
    "             x['ARG_OF_PERICENTER'], # argpo: argument of perigee (radians)\n",
    "             x['INCLINATION'], # inclo: inclination (radians)\n",
    "             x['MEAN_ANOMALY'], # mo: mean anomaly (radians)\n",
    "             x['MEAN_MOTION'], # no_kozai: mean motion (radians/minute)\n",
    "             x['RA_OF_ASC_NODE'], # nodeo: right ascension of ascending node (radians)\n",
    "        )\n",
    "        return np.array(sat.sgp4(x['epoch_jd'], x['epoch_fr'])[1])\n",
    "\n",
    "    # Join our results with the y_test column data\n",
    "    y_test_error = pd.DataFrame(results, columns=y_test.columns[:-4]) \\\n",
    "                     .merge(y_test.reset_index()[['epoch_days', 'epoch_jd', 'epoch_fr', 'satpos']],\n",
    "                            left_index=True, right_index=True)\n",
    "    \n",
    "    # Convert columns to radians\n",
    "    cols_to_radians = ['RA_OF_ASC_NODE', 'MEAN_ANOMALY', 'INCLINATION', 'ARG_OF_PERICENTER']\n",
    "    y_test_error[cols_to_radians] = y_test_error[cols_to_radians]*np.pi/180\n",
    "    y_test_error['MEAN_MOTION'] = y_test_error['MEAN_MOTION']*np.pi/(4*180)\n",
    "    \n",
    "    # Calculate position based on predicted values\n",
    "    y_test_error['satpos_calc'] = y_test_error.apply(get_satpos, axis=1)\n",
    "    \n",
    "    # Get the error between calculated position and TLE position\n",
    "    y_test_error['pos_predict_error'] = y_test_error.apply(get_mserror, axis=1)\n",
    "    \n",
    "    return y_test_error\n",
    "\n",
    "y_predict_error = get_predicted_error(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_error['pos_predict_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propigated_error():\n",
    "    '''\n",
    "    This returns the Mean-Squared Error\n",
    "    of the propigated TLE's satellite position\n",
    "    vs the actional TLE's satellites position\n",
    "    '''\n",
    "    def get_satpos(x):\n",
    "        return np.array(x.satobj.sgp4(x.target_epoch_jd, x.target_epoch_fr)[1])\n",
    "\n",
    "    def get_mserror(x):\n",
    "        return ((x['satpos']-x['satpos_prop'])**2).mean()\n",
    "\n",
    "    X_propigation_error = X_all.loc[X_test.index]\n",
    "    X_propigation_error['satpos_prop'] = X_propigation_error.apply(get_satpos, axis=1)\n",
    "    X_propigation_error['pos_propigate_error'] = X_propigation_error.apply(get_mserror, axis=1)\n",
    "    \n",
    "    return X_propigation_error\n",
    "\n",
    "X_prop_error = get_propigated_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prop_error['pos_propigate_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean Error in Propigation {np.sqrt(X_prop_error.pos_propigate_error.mean())}')\n",
    "print(f'Mean Error in Prediction {np.sqrt(y_predict_error.pos_predict_error.mean())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
