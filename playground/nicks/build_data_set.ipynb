{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sgp4.api import Satrec, WGS72\n",
    "from sgp4.conveniences import jday_datetime\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the norads used in training\n",
    "train_norad_df = pd.read_pickle( os.environ['my_home_path'] + '/data/split_by_norad/train_norads.pkl.gz')\n",
    "train_norad_list = train_norad_df.norad.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#csv_store_path = os.environ['GP_HIST_PATH']\n",
    "csv_store_path = os.environ['my_home_path'] + '/data/space-track-gp-hist-sample'\n",
    "\n",
    "necessary_columns = ['NORAD_CAT_ID','OBJECT_TYPE','OBJECT_NAME','TLE_LINE1','TLE_LINE2',\n",
    "                     'MEAN_MOTION_DOT', 'MEAN_MOTION_DDOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE',\n",
    "                     'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY', 'MEAN_MOTION', 'EPOCH']\n",
    "\n",
    "dfs = None\n",
    "files = sorted([x for x in os.listdir(f'{csv_store_path}/') if x.endswith(\".csv.gz\")])\n",
    "for f in tqdm(files):\n",
    "    #df = pd.read_csv(f'{csv_store_path}/{f}', parse_dates=['EPOCH'], infer_datetime_format=True, index_col='EPOCH', compression='gzip')\n",
    "    df = pd.read_csv(f'{csv_store_path}/{f}', parse_dates=['EPOCH'], infer_datetime_format=True, compression='gzip')\n",
    "    # LEO = Mean Motion > 11.25 and Eccentricity < 0.25\n",
    "    #df = df[(df.MEAN_MOTION > 11.25) & (df.ECCENTRICITY < 0.25)]\n",
    "    df = df[df.NORAD_CAT_ID.isin(train_norad_list)][necessary_columns]\n",
    "\n",
    "    # Since animated gabbard diagrams are generated per frame, we can revert the scaling when we plot the graphs\n",
    "    if dfs is None:\n",
    "        dfs = df\n",
    "    elif len(df) > 0:\n",
    "        dfs = pd.concat([dfs,df])\n",
    "            \n",
    "# Remove unnecessary columns to save memory\n",
    "# unnecessary_columns = ['CCSDS_OMM_VERS', 'COMMENT', 'CREATION_DATE', 'ORIGINATOR', 'OBJECT_NAME', 'OBJECT_ID',\n",
    "#                        'CENTER_NAME', 'REF_FRAME', 'TIME_SYSTEM', 'MEAN_ELEMENT_THEORY', 'EPHEMERIS_TYPE',\n",
    "#                        'CLASSIFICATION_TYPE', 'ELEMENT_SET_NO', 'REV_AT_EPOCH', 'SEMIMAJOR_AXIS', 'PERIOD',\n",
    "#                        'APOAPSIS', 'PERIAPSIS', 'OBJECT_TYPE', 'RCS_SIZE', 'COUNTRY_CODE', 'LAUNCH_DATE',\n",
    "#                        'SITE', 'DECAY_DATE', 'FILE', 'GP_ID', 'TLE_LINE0', 'index']\n",
    "# dfs = dfs.reset_index().drop(columns=unnecessary_columns, axis=1)\n",
    "dfs = dfs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satellite_position_data():\n",
    "    # Create the satellite object (used to find satellite position)\n",
    "    dfs['satobj'] = dfs.apply(lambda x: Satrec.twoline2rv(x['TLE_LINE1'], x['TLE_LINE2']), axis=1)\n",
    "\n",
    "    # Get the Julian date of the EPOCH\n",
    "    #dfs['epoch_julian'] = dfs['EPOCH'].apply(lambda x: jday_datetime(x.replace(tzinfo=timezone.utc)))\n",
    "    dfs[['epoch_jd', 'epoch_fr']] = dfs['EPOCH'].apply(lambda x: jday_datetime(x.replace(tzinfo=timezone.utc))).to_list()\n",
    "\n",
    "    # Get the days since 1949 December 31 00:00 UT\n",
    "    # This will be used when creating satobj for the test set\n",
    "    # (this is needed to get the satellite position from generated TLEs\n",
    "    #  because of how Satrec sgp4init() works')\n",
    "    ref_date = datetime.strptime('12/31/1949 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "    dfs['epoch_days'] = dfs['EPOCH'].apply(lambda x: (x-ref_date)/np.timedelta64(1, 'D'))\n",
    "\n",
    "    # Get satellite x,y,z positions from TLE\n",
    "    #dfs['satpos'] = dfs.apply(lambda x: x['satobj'].sgp4(*x['epoch_julian'])[1], axis=1)\n",
    "    dfs['satpos'] = dfs.apply(lambda x: np.array(x['satobj'].sgp4(x['epoch_jd'], x['epoch_fr'])[1]), axis=1)\n",
    "\n",
    "get_satellite_position_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xy():\n",
    "    \n",
    "    # ML Structure\n",
    "    # Input:\n",
    "    #  - Reference TLE Data (+ EPOCH)\n",
    "    #  - Target EPOCH\n",
    "    # Output:\n",
    "    #  - Target TLE Data\n",
    "    \n",
    "    def groups(lst):\n",
    "        arr = lst.copy()\n",
    "        np.random.shuffle(arr)\n",
    "        i=1\n",
    "        if len(lst)<=1:\n",
    "            return\n",
    "        while True:\n",
    "            if i==len(lst):\n",
    "                yield tuple((arr[i-1],arr[0]))\n",
    "                break\n",
    "            else:\n",
    "                yield tuple((arr[i-1],arr[i]))\n",
    "                i+=1\n",
    "    \n",
    "    # For each unique NORAD, find all TLE indexes and generate\n",
    "    # a list of combinations\n",
    "    idx_pairs = []\n",
    "    for norad in dfs['NORAD_CAT_ID'].unique():\n",
    "        norad_idxs = dfs[dfs['NORAD_CAT_ID']==norad].index.values\n",
    "        if len(norad_idxs > 1):\n",
    "            idx_pairs.extend(groups(norad_idxs))\n",
    "    idx_pairs = np.array(idx_pairs)\n",
    "    \n",
    "    # Build our X/Y datasets\n",
    "    X_all = dfs.loc[idx_pairs[:,0]].reset_index()\n",
    "    Y_all = dfs.loc[idx_pairs[:,1]].reset_index()\n",
    "    \n",
    "    # This will be the column that links x and y\n",
    "    key_columns = ['epoch_jd', 'epoch_fr']\n",
    "    target_columns = ['target_epoch_jd', 'target_epoch_fr']\n",
    "    X_all[target_columns] = Y_all[key_columns]\n",
    "    \n",
    "    return X_all, Y_all\n",
    "\n",
    "X_all, y_all = create_xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_X(X):\n",
    "    # Perform any cleaning of values\n",
    "    \n",
    "    # Return only necessary columns\n",
    "    X_columns = ['MEAN_MOTION_DOT', 'MEAN_MOTION_DDOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE',\n",
    "                 'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY', 'MEAN_MOTION',\n",
    "                 'epoch_jd', 'epoch_fr', 'target_epoch_jd', 'target_epoch_fr']\n",
    "    \n",
    "    return X[X_columns]\n",
    "    \n",
    "X_all_clean = clean_X(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Y(Y):\n",
    "    # Perform any cleaning of values\n",
    "    \n",
    "    # Return only necessary columns\n",
    "    Y_columns = ['MEAN_MOTION_DOT', 'MEAN_MOTION_DDOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE',\n",
    "                 'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY', 'MEAN_MOTION',\n",
    "                 'epoch_days', 'epoch_jd', 'epoch_fr', 'satpos']\n",
    "    \n",
    "    return Y[Y_columns]\n",
    "    \n",
    "y_all_clean = clean_Y(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_clean, y_all_clean, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "# Remove non-training columns from y_train\n",
    "non_training_cols = ['epoch_days', 'epoch_jd', 'epoch_fr', 'satpos']\n",
    "y_train = y_train.drop(columns=non_training_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra trees got score 0.6115793089284733\n",
      "Random Forest got score 0.5703758460606542\n",
      "K-nn got score 0.5065872838459986\n",
      "Linear regression got score 0.3948084251322881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "y_test = y_test.drop(columns=non_training_cols, axis=1)\n",
    "\n",
    "ESTIMATORS = {\n",
    "    \"Extra trees\": ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=10, random_state=0, n_jobs=-1),\n",
    "    #\"GradientBoost\": MultiOutputRegressor(GradientBoostingRegressor(n_estimators=10, random_state=0)),\n",
    "    \"K-nn\": KNeighborsRegressor(n_jobs=-1),\n",
    "    \"Linear regression\": LinearRegression(n_jobs=-1),\n",
    "    #\"Ridge\": RidgeCV(),\n",
    "    #\"SVM\": MultiOutputRegressor(SVR()),\n",
    "}\n",
    "\n",
    "# Let's see how it does on the same NORAD\n",
    "for name, estimator in ESTIMATORS.items():\n",
    "    estimator.fit(X_train, y_train)\n",
    "    score = estimator.score(X_test, y_test)\n",
    "    print (f'{name} got score {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterator.combinations()\n",
    "```\n",
    "Extra trees got score 0.9080013082091861\n",
    "Random Forest got score 0.883921000656464\n",
    "GradientBoost got score 0.5516253957176365\n",
    "K-nn got score 0.7644015286756071\n",
    "Linear regression got score 0.4746575589454271\n",
    "Ridge got score 0.4744900285859706\n",
    "```\n",
    "groups()\n",
    "```\n",
    "Extra trees got score 0.534262241603748\n",
    "Random Forest got score 0.4692323383272387\n",
    "GradientBoost got score 0.356691868881702\n",
    "K-nn got score 0.36862432289895664\n",
    "Linear regression got score 0.4765033608802853\n",
    "Ridge got score 0.45090731103313353\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEAN_MOTION_DOT     -4.000000e-08\n",
       "MEAN_MOTION_DDOT     0.000000e+00\n",
       "BSTAR                1.000000e-04\n",
       "INCLINATION          7.406290e+01\n",
       "RA_OF_ASC_NODE       1.151074e+02\n",
       "ECCENTRICITY         1.082020e-02\n",
       "ARG_OF_PERICENTER    2.546340e+02\n",
       "MEAN_ANOMALY         1.042746e+02\n",
       "MEAN_MOTION          1.227767e+01\n",
       "epoch_jd             2.452040e+06\n",
       "epoch_fr             9.721857e-01\n",
       "target_epoch_jd      2.452042e+06\n",
       "target_epoch_fr      9.498902e-01\n",
       "Name: 111753, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(X_train.to_numpy()).float()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "hiddenSize = 300\n",
    "batchSize = 200\n",
    "learningRate = 0.01\n",
    "numEpochs = 10\n",
    "\n",
    "# Help with cuda: https://discuss.pytorch.org/t/how-to-load-all-data-into-gpu-for-training/27609\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu') # 3min 46s / 3min 54s / 15.6 s\n",
    "#device = torch.device('cuda') # 12.1 s\n",
    "#device = torch.device('cuda:0')\n",
    "\n",
    "# Another idea is to use parallelism instead of GPU: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "# for inputs, labels in trainLoader:\n",
    "#     inputs = to_device(inputs, device)\n",
    "#     break\n",
    "\n",
    "# Build Dataset and DataLoader: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, X_data, y_data, device='cpu'):\n",
    "        'Initialization'\n",
    "        self.y_data = to_device(torch.from_numpy(y_data.to_numpy()).float(), device)\n",
    "        self.X_data = to_device(torch.from_numpy(X_data.to_numpy()).float(), device)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample and convert from pandas to numpy to tensor\n",
    "        X = self.X_data[index]\n",
    "        y = self.y_data[index]\n",
    "        \n",
    "        # Send to GPU\n",
    "        #X = to_device(X, self.device)\n",
    "        #y = to_device(y, self.device)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, hiddenSize, activate=None):\n",
    "        super().__init__()\n",
    "        self.activate = nn.Sigmoid() if activate == \"Sigmoid\" else nn.Tanh() if activate == \"Tanh\" else nn.ReLU()\n",
    "        self.layer1 = nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = nn.Linear(hiddenSize, outputSize)\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden = self.activate(self.layer1(X))\n",
    "        return self.layer2(hidden)\n",
    "        \n",
    "        \n",
    "net = NNModel(len(X_train.columns), len(y_train.columns), hiddenSize)\n",
    "to_device(net, device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learningRate)\n",
    "\n",
    "\n",
    "trainDataset = Dataset(X_train, y_train, device)\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=trainDataset,\n",
    "                                          batch_size=batchSize,\n",
    "                                          shuffle=True,\n",
    "                                          #pin_memory=True,\n",
    "                                         )\n",
    "# Test will be done by other means...see below\n",
    "# testDataset = Dataset(X_test, y_test.iloc[:,:len(y_train.columns)], device)\n",
    "# testLoader = torch.utils.data.DataLoader(dataset=testDataset,\n",
    "#                                          batch_size=batchSize, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Beginning training!\n",
      "Epoch [1/10], Step [100/635], Loss: 3146.41943359375\n",
      "Epoch [1/10], Step [200/635], Loss: 1181.62646484375\n",
      "Epoch [1/10], Step [300/635], Loss: 69.24398803710938\n",
      "Epoch [1/10], Step [400/635], Loss: 69.44286346435547\n",
      "Epoch [1/10], Step [500/635], Loss: 68.45832061767578\n",
      "Epoch [1/10], Step [600/635], Loss: 68.99266815185547\n",
      "Epoch [2/10], Step [100/635], Loss: 66.46878814697266\n",
      "Epoch [2/10], Step [200/635], Loss: 67.81497192382812\n",
      "Epoch [2/10], Step [300/635], Loss: 69.0251693725586\n",
      "Epoch [2/10], Step [400/635], Loss: 65.7192153930664\n",
      "Epoch [2/10], Step [500/635], Loss: 64.6150131225586\n",
      "Epoch [2/10], Step [600/635], Loss: 66.43534088134766\n",
      "Epoch [3/10], Step [100/635], Loss: 64.3507308959961\n",
      "Epoch [3/10], Step [200/635], Loss: 63.36769104003906\n",
      "Epoch [3/10], Step [300/635], Loss: 63.13770294189453\n",
      "Epoch [3/10], Step [400/635], Loss: 62.3521728515625\n",
      "Epoch [3/10], Step [500/635], Loss: 61.92041015625\n",
      "Epoch [3/10], Step [600/635], Loss: 61.53953552246094\n",
      "Epoch [4/10], Step [100/635], Loss: 61.2278938293457\n",
      "Epoch [4/10], Step [200/635], Loss: 60.77810287475586\n",
      "Epoch [4/10], Step [300/635], Loss: 60.44599151611328\n",
      "Epoch [4/10], Step [400/635], Loss: 59.056251525878906\n",
      "Epoch [4/10], Step [500/635], Loss: 59.50349044799805\n",
      "Epoch [4/10], Step [600/635], Loss: 59.30062484741211\n",
      "Epoch [5/10], Step [100/635], Loss: 59.20344161987305\n",
      "Epoch [5/10], Step [200/635], Loss: 58.89010238647461\n",
      "Epoch [5/10], Step [300/635], Loss: 59.06478500366211\n",
      "Epoch [5/10], Step [400/635], Loss: 57.94750213623047\n",
      "Epoch [5/10], Step [500/635], Loss: 57.10871505737305\n",
      "Epoch [5/10], Step [600/635], Loss: 56.87145233154297\n",
      "Epoch [6/10], Step [100/635], Loss: 57.13262176513672\n",
      "Epoch [6/10], Step [200/635], Loss: 55.202354431152344\n",
      "Epoch [6/10], Step [300/635], Loss: 55.82195281982422\n",
      "Epoch [6/10], Step [400/635], Loss: 55.43022537231445\n",
      "Epoch [6/10], Step [500/635], Loss: 53.980323791503906\n",
      "Epoch [6/10], Step [600/635], Loss: 54.298118591308594\n",
      "Epoch [7/10], Step [100/635], Loss: 53.729942321777344\n",
      "Epoch [7/10], Step [200/635], Loss: 54.28734588623047\n",
      "Epoch [7/10], Step [300/635], Loss: 53.626651763916016\n",
      "Epoch [7/10], Step [400/635], Loss: 52.696475982666016\n",
      "Epoch [7/10], Step [500/635], Loss: 52.73722457885742\n",
      "Epoch [7/10], Step [600/635], Loss: 53.70515823364258\n",
      "Epoch [8/10], Step [100/635], Loss: 52.948246002197266\n",
      "Epoch [8/10], Step [200/635], Loss: 50.50484848022461\n",
      "Epoch [8/10], Step [300/635], Loss: 52.095096588134766\n",
      "Epoch [8/10], Step [400/635], Loss: 50.69062805175781\n",
      "Epoch [8/10], Step [500/635], Loss: 50.98065185546875\n",
      "Epoch [8/10], Step [600/635], Loss: 49.7624397277832\n",
      "Epoch [9/10], Step [100/635], Loss: 50.88098907470703\n",
      "Epoch [9/10], Step [200/635], Loss: 49.367942810058594\n",
      "Epoch [9/10], Step [300/635], Loss: 49.03257369995117\n",
      "Epoch [9/10], Step [400/635], Loss: 48.56993103027344\n",
      "Epoch [9/10], Step [500/635], Loss: 48.71867752075195\n",
      "Epoch [9/10], Step [600/635], Loss: 49.533958435058594\n",
      "Epoch [10/10], Step [100/635], Loss: 49.213871002197266\n",
      "Epoch [10/10], Step [200/635], Loss: 47.74098205566406\n",
      "Epoch [10/10], Step [300/635], Loss: 48.01345443725586\n",
      "Epoch [10/10], Step [400/635], Loss: 47.33638000488281\n",
      "Epoch [10/10], Step [500/635], Loss: 47.13503646850586\n",
      "Epoch [10/10], Step [600/635], Loss: 47.50278854370117\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('>>> Beginning training!')\n",
    "for epoch in range(numEpochs):\n",
    "    for i, (inputs, labels) in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward propagation\n",
    "        outputs = net(inputs)\n",
    "        # Backpropagation\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Gradient descent\n",
    "        optimizer.step()\n",
    "        # Logging\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {}'.format(epoch+1, numEpochs, i+1,\n",
    "                                                                 len(trainDataset)//batchSize, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('>>> Beginning validation!')\n",
    "# correct, total = 0, 0\n",
    "# for i, (inputs, labels) in enumerate(testLoader):\n",
    "#     outputs = net(inputs)\n",
    "#     _, prediction = torch.max(outputs, axis=1)\n",
    "#     correct += torch.sum(prediction == labels)\n",
    "#     total += labels.size(0)\n",
    "# print('Validation accuracy: {}%'.format(correct.item()/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62617, 9)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the test set to GPU and get the results then move the results back to the CPU as numpy array\n",
    "X_test_tensor = to_device(torch.from_numpy(X_test.to_numpy()).float(), device)\n",
    "nn_results = net(X_test_tensor).detach().cpu().numpy()\n",
    "nn_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180.1561"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.loc[0,'RA_OF_ASC_NODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#model = ExtraTreesRegressor(n_estimators=10, random_state=0, n_jobs=-1).fit(X_train, y_train)\n",
    "model = LinearRegression(n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62617, 9)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.predict(X_test)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy version of SGP4:\n",
    "https://github.com/aerospaceresearch/orbitdeterminator/blob/master/orbitdeterminator/propagation/sgp4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_error(results):\n",
    "    '''\n",
    "    This returns the Mean-Squared Error\n",
    "    of the predicted TLE's satellite position\n",
    "    vs the actional TLE's satellites position\n",
    "    '''\n",
    "    \n",
    "    def get_mserror(x):\n",
    "        return ((x['satpos']-x['satpos_calc'])**2).mean()\n",
    "\n",
    "    def get_satpos(x):\n",
    "        sat = Satrec()\n",
    "        sat.sgp4init(\n",
    "             WGS72,           # gravity model\n",
    "             'i',             # 'a' = old AFSPC mode, 'i' = improved mode\n",
    "             0,               # satnum: Satellite number\n",
    "             x['epoch_days'],       # epoch: days since 1949 December 31 00:00 UT\n",
    "             x['BSTAR'],      # bstar: d`rag coefficient (/earth radii)\n",
    "             x['MEAN_MOTION_DOT'], # ndot (NOT USED): ballistic coefficient (revs/day)\n",
    "             x['MEAN_MOTION_DDOT'],             # nddot (NOT USED): mean motion 2nd derivative (revs/day^3)\n",
    "             x['ECCENTRICITY'],       # ecco: eccentricity\n",
    "             x['ARG_OF_PERICENTER'], # argpo: argument of perigee (radians)\n",
    "             x['INCLINATION'], # inclo: inclination (radians)\n",
    "             x['MEAN_ANOMALY'], # mo: mean anomaly (radians)\n",
    "             x['MEAN_MOTION'], # no_kozai: mean motion (radians/minute)\n",
    "             x['RA_OF_ASC_NODE'], # nodeo: right ascension of ascending node (radians)\n",
    "        )\n",
    "        return np.array(sat.sgp4(x['epoch_jd'], x['epoch_fr'])[1])\n",
    "\n",
    "    # Join our results with the y_test column data\n",
    "    y_test_error = pd.DataFrame(results, columns=y_test.columns[:-4]) \\\n",
    "                     .merge(y_test.reset_index()[['epoch_days', 'epoch_jd', 'epoch_fr', 'satpos']],\n",
    "                            left_index=True, right_index=True)\n",
    "    \n",
    "    # Convert columns to radians\n",
    "    cols_to_radians = ['RA_OF_ASC_NODE', 'MEAN_ANOMALY', 'INCLINATION', 'ARG_OF_PERICENTER']\n",
    "    y_test_error[cols_to_radians] = y_test_error[cols_to_radians]*np.pi/180\n",
    "    y_test_error['MEAN_MOTION'] = y_test_error['MEAN_MOTION']*np.pi/(4*180)\n",
    "    \n",
    "    # Calculate position based on predicted values\n",
    "    y_test_error['satpos_calc'] = y_test_error.apply(get_satpos, axis=1)\n",
    "    \n",
    "    # Get the error between calculated position and TLE position\n",
    "    y_test_error['pos_predict_error'] = y_test_error.apply(get_mserror, axis=1)\n",
    "    \n",
    "    return y_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.929700e+04\n",
       "mean     2.536345e+07\n",
       "std      5.935988e+07\n",
       "min      5.760510e+01\n",
       "25%      2.691235e+06\n",
       "50%      1.239972e+07\n",
       "75%      3.095732e+07\n",
       "max      2.200150e+09\n",
       "Name: pos_predict_error, dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_error = get_predicted_error(results)\n",
    "y_nn_pred_error = get_predicted_error(nn_results)\n",
    "y_predict_error['pos_predict_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propigated_error():\n",
    "    '''\n",
    "    This returns the Mean-Squared Error\n",
    "    of the propigated TLE's satellite position\n",
    "    vs the actional TLE's satellites position\n",
    "    '''\n",
    "    def get_satpos(x):\n",
    "        return np.array(x.satobj.sgp4(x.target_epoch_jd, x.target_epoch_fr)[1])\n",
    "\n",
    "    def get_mserror(x):\n",
    "        return ((x['satpos']-x['satpos_prop'])**2).mean()\n",
    "\n",
    "    X_propigation_error = X_all.loc[X_test.index]\n",
    "    X_propigation_error['satpos_prop'] = X_propigation_error.apply(get_satpos, axis=1)\n",
    "    X_propigation_error['pos_propigate_error'] = X_propigation_error.apply(get_mserror, axis=1)\n",
    "    \n",
    "    return X_propigation_error\n",
    "\n",
    "X_prop_error = get_propigated_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.254500e+04\n",
       "mean     3.161009e+34\n",
       "std      7.905365e+36\n",
       "min      2.149102e-01\n",
       "25%      3.260486e+05\n",
       "50%      2.411628e+06\n",
       "75%      2.582770e+07\n",
       "max      1.977053e+39\n",
       "Name: pos_propigate_error, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prop_error['pos_propigate_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error in Propigation 1.777922663430849e+17\n",
      "Mean Error in Prediction 5036.214051855435\n",
      "Mean Error in Prediction 6062.998220039357\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Error in Propigation {np.sqrt(X_prop_error.pos_propigate_error.mean())}')\n",
    "print(f'Mean Error in Prediction {np.sqrt(y_predict_error.pos_predict_error.mean())}')\n",
    "print(f'Mean Error in Prediction {np.sqrt(y_nn_pred_error.pos_predict_error.mean())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                            59338\n",
       "MEAN_MOTION_DOT                                               0.000423\n",
       "MEAN_MOTION_DDOT                                              0.000003\n",
       "BSTAR                                                           0.0093\n",
       "INCLINATION                                                   1.718075\n",
       "RA_OF_ASC_NODE                                                3.780205\n",
       "ECCENTRICITY                                                  0.003082\n",
       "ARG_OF_PERICENTER                                              1.97072\n",
       "MEAN_ANOMALY                                                  4.320189\n",
       "MEAN_MOTION                                                   0.064617\n",
       "epoch_days                                                18758.633707\n",
       "epoch_jd                                                     2452039.5\n",
       "epoch_fr                                                      0.633707\n",
       "satpos               [-5622.023417389518, -4185.879322657325, 5.324...\n",
       "satpos_calc          [-5633.176636682518, -4182.109342750234, -0.52...\n",
       "pos_predict_error                                            57.605097\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best performing satpos prediction - LinearRegression\n",
    "# satpos - The x,y,z based on actual TLE and epoch (same as from y_test)\n",
    "# satpos_calc - The x,y,z calculated based on the ML model\n",
    "y_predict_error.sort_values(by='pos_predict_error').reset_index().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEAN_MOTION_DOT                                               0.000841\n",
       "MEAN_MOTION_DDOT                                                   0.0\n",
       "BSTAR                                                         0.011192\n",
       "INCLINATION                                                    98.4392\n",
       "RA_OF_ASC_NODE                                                 216.676\n",
       "ECCENTRICITY                                                  0.003657\n",
       "ARG_OF_PERICENTER                                              78.8616\n",
       "MEAN_ANOMALY                                                  281.7141\n",
       "MEAN_MOTION                                                  14.787814\n",
       "epoch_days                                                18758.633707\n",
       "epoch_jd                                                     2452039.5\n",
       "epoch_fr                                                      0.633707\n",
       "satpos               [-5622.023417389518, -4185.879322657325, 5.324...\n",
       "Name: 183865, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The actual result\n",
    "# satpos - The x,y,z based on actual TLE and epoch\n",
    "y_test.iloc[59338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEAN_MOTION_DOT                                              -0.000811\n",
       "MEAN_MOTION_DDOT                                             -0.001248\n",
       "BSTAR                                                         -0.00093\n",
       "INCLINATION                                                   1.036004\n",
       "RA_OF_ASC_NODE                                                1.020209\n",
       "ECCENTRICITY                                                  0.005052\n",
       "ARG_OF_PERICENTER                                             1.011387\n",
       "MEAN_ANOMALY                                                  1.011653\n",
       "MEAN_MOTION                                                   0.060026\n",
       "epoch_days                                                18758.633707\n",
       "epoch_jd                                                     2452039.5\n",
       "epoch_fr                                                      0.633707\n",
       "satpos               [-5622.023417389518, -4185.879322657325, 5.324...\n",
       "satpos_calc          [-4560.803892385173, -1031.3778705869129, 5647...\n",
       "pos_predict_error                                      14301985.502275\n",
       "Name: 59338, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The NN result\n",
    "y_nn_pred_error.loc[59338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5642.35108646, -4142.41691223,   331.56394272])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The x,y,z based on propigationa\n",
    "X_prop_error.iloc[59338]['satpos_prop']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
