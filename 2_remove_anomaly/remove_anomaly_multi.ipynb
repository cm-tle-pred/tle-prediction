{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Abnormal Data from Training Set\n",
    "\n",
    "Load DataFrame from `1_min/train.pkl`\n",
    "\n",
    "Define multile functions that take a dataframe and return a boolean mask for entries to keep.\n",
    "\n",
    "Combine masks in the end into a final mask, then it can be applied to `1_full` version as well if that's preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('my_home_path' not in os.environ) and ('MY_HOME_PATH' in os.environ):\n",
    "    os.environ['my_home_path'] = os.environ['MY_HOME_PATH'] # because stupid :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../1_min/train.pkl\")\n",
    "\n",
    "# total unique NORAD_IDs\n",
    "# len(train_df.NORAD_CAT_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(train_df.NORAD_CAT_ID.unique(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using a small subset for testing\n",
    "# train_backup_df = train_df.copy()\n",
    "# # sample_ids = [12223, 26285, 10760, 14345, 34588, 330, 20970]\n",
    "# sample_ids = [12223, 26285, 10760, 14345, 34588, 330, 20970, 35253, 38899, 36390, 27507, 31539,  8386,  6299, 18428, 17228, 42126]\n",
    "# sample_ids += list(np.random.choice(train_df.NORAD_CAT_ID.unique(),20))\n",
    "# # sample_ids = [28974,36024,24403] # this is for arg of pericenter fails\n",
    "# train_df = train_backup_df[train_backup_df.NORAD_CAT_ID.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Erroneous Data\n",
    "First we start by removing data which we don't want to include in our model.  This includes values which are outside of acceptable ranges or are physically impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early TLEs are more prone to errors, cut off should be somewhere in the 80s\n",
    "# TODO: putting in 1990 for now to be on the safe side\n",
    "\n",
    "def more_recent_only(df):\n",
    "    mask = df.EPOCH > \"1990\"\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space track LEO definition: Mean Motion > 11.25 and Eccentricity < 0.25\n",
    "# This means that satellite that decay into LEO will not have non-LEO-like entries removed\n",
    "\n",
    "def leo_check(df):\n",
    "    mask = (df['MEAN_MOTION'] > 11.25) & (df['ECCENTRICITY'] < 0.25)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid range for 'RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY' is 0..360\n",
    "\n",
    "def degrees_range_check(df):\n",
    "    degree_columns = ['RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY']\n",
    "    mask = df[degree_columns].apply(lambda x:x.between(0,360), axis=0).all(axis=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid INCLINATION range is 0..180\n",
    "\n",
    "def inclination_range_check(df):\n",
    "    mask = df['INCLINATION'].between(0,180)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Anything beyond 20 should be outliers... I think...\n",
    "# > 16.5 you do get multiple entries from the same satellite, so those shouldn't be outliers\n",
    "# train_df[train_df['MEAN_MOTION'] > 16.5].NORAD_CAT_ID.value_counts()    \n",
    "\n",
    "def mean_motion_range_check(df):\n",
    "    mask = df['MEAN_MOTION'].between(11.25,20)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping first few entries may be a good idea due to initial readings being less accurate (?)\n",
    "# TODO: using N=5 for now\n",
    "# this one takes longer because of the grouping\n",
    "\n",
    "def skip_first_n(df, n=5):\n",
    "    mask = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).apply(lambda x:x.EPOCH.rank() > n).reset_index(level=0, drop=True).sort_index()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc088476c514d2ab186af503a5c5afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing function: \"more_recent_only\"\n",
      "CPU times: user 164 ms, sys: 24.7 ms, total: 188 ms\n",
      "Wall time: 188 ms\n",
      "==========================================================\n",
      "Processing function: \"leo_check\"\n",
      "CPU times: user 281 ms, sys: 61.4 ms, total: 342 ms\n",
      "Wall time: 106 ms\n",
      "==========================================================\n",
      "Processing function: \"degrees_range_check\"\n",
      "CPU times: user 1.11 s, sys: 600 ms, total: 1.71 s\n",
      "Wall time: 918 ms\n",
      "==========================================================\n",
      "Processing function: \"inclination_range_check\"\n",
      "CPU times: user 305 ms, sys: 58.4 ms, total: 364 ms\n",
      "Wall time: 103 ms\n",
      "==========================================================\n",
      "Processing function: \"mean_motion_range_check\"\n",
      "CPU times: user 276 ms, sys: 70.6 ms, total: 346 ms\n",
      "Wall time: 102 ms\n",
      "==========================================================\n",
      "Processing function: \"skip_first_n\"\n",
      "CPU times: user 49.5 s, sys: 8.76 s, total: 58.3 s\n",
      "Wall time: 53.5 s\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "anomaly_functions = [\n",
    "    more_recent_only,\n",
    "    leo_check,\n",
    "    degrees_range_check,\n",
    "    inclination_range_check,\n",
    "    mean_motion_range_check,\n",
    "    skip_first_n,\n",
    "]\n",
    "\n",
    "anomaly_results = []\n",
    "for fn in tqdm(anomaly_functions):\n",
    "    print(f\"Processing function: \\\"{fn.__name__}\\\"\")\n",
    "    %time res = fn(train_df)\n",
    "    print(\"==========================================================\")\n",
    "    res.name = fn.__name__\n",
    "    anomaly_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     50453855\n",
       "False     4785984\n",
       "Name: more_recent_only, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     54678326\n",
       "False      561513\n",
       "Name: leo_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     55239837\n",
       "False           2\n",
       "Name: degrees_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True    55239839\n",
       "Name: inclination_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     54678335\n",
       "False      561504\n",
       "Name: mean_motion_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     55167118\n",
       "False       72721\n",
       "Name: skip_first_n, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Masks combined:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     49901955\n",
       "False     5337884\n",
       "Name: combined_masks, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mask results\n",
    "\n",
    "for s in anomaly_results:\n",
    "    display(s.value_counts())\n",
    "    \n",
    "combined = pd.concat(anomaly_results, axis=1).all(axis=1)\n",
    "combined.name = \"combined_masks\"\n",
    "print(\"==========================================================\\nMasks combined:\")\n",
    "display(combined.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a new DataFrame for Outliers\n",
    "\n",
    "Masked version of DataFrame for unsupervised learning outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = train_df[combined]\n",
    "# masked_sample_df = masked_df[masked_df.NORAD_CAT_ID.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Unsupervised Learning to Remove Outliers\n",
    "\n",
    "We'll be using `DBSCAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # testing DBSCAN\n",
    "\n",
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# def dbscan_removal(df, debug=False):\n",
    "# #     columns = [\"INCLINATION\",\"ECCENTRICITY\",\"MEAN_MOTION\"]\n",
    "#     # mean motion turns out to be not very good, due to the final decay as well as outliers reflected in other fields as well\n",
    "#     columns = [\"INCLINATION\",\"ECCENTRICITY\"]\n",
    "\n",
    "#     def detect_outliers(input_df):\n",
    "#         name = input_df.name\n",
    "#         dbscan_min_samples = max(len(input_df)/100, 20)\n",
    "        \n",
    "#         sub_df = input_df.set_index('EPOCH', append=True).sort_index(level=1)\n",
    "#         outlier_labels = []\n",
    "#         for i,column in enumerate(columns):\n",
    "#             col_diff = np.minimum(sub_df[column].diff()**2, sub_df[column].diff(-1)**2).fillna(0) + np.minimum(sub_df[column].diff(2)**2, sub_df[column].diff(-2)**2).fillna(0)\n",
    "#             dbscan_eps = col_diff.std()*3\n",
    "#             if not dbscan_eps > 0.0: # should never or rarely happen, but has happened before....\n",
    "#                 dbscan_eps = 1 # arbitary, which should mean no outliers for this satellite\n",
    "#                 dbscan_eps_zero_neg.append(name) # keep track of it\n",
    "            \n",
    "#             db = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(col_diff.to_frame())\n",
    "#             core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#             core_samples_mask[db.core_sample_indices_] = True\n",
    "#             labels = db.labels_\n",
    "#             # Number of clusters in labels, ignoring noise if present.\n",
    "#             outlier_labels.append(labels)\n",
    "\n",
    "#         all_normal = (np.array(outlier_labels).T.min(axis=1) != -1)\n",
    "\n",
    "#         normal_data = sub_df[all_normal]\n",
    "\n",
    "#         if debug:\n",
    "#             print(f\"=============================\\nnorad id: {name}, rows:{len(input_df)}\")\n",
    "#             ax = (sub_df[columns].droplevel(0)).plot(subplots=True,figsize=(20,6));\n",
    "#             outlier_data = sub_df[~all_normal]\n",
    "#             num_all_outliers = len(input_df)-np.sum(all_normal)\n",
    "#             for i,column in enumerate(columns):\n",
    "#                 labels = outlier_labels[i]\n",
    "#                 n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "#                 n_noise_ = list(labels).count(-1)    \n",
    "#                 print(f\"column: {column}, n_clusters_: {n_clusters_}, n_noise_: {n_noise_}, noise %:{n_noise_/len(input_df):.5f}%\")\n",
    "\n",
    "#                 ax[i].scatter(outlier_data.index.get_level_values(1), outlier_data[column], s=40, color=\"red\", alpha=1, marker=\"x\", zorder=-1)\n",
    "#     #             ax[i].set_title=f\"{column} #clusters: {n_clusters_}, #noise: {n_noise_}, noise %:{n_noise_/len(input_df):.5f}%\"\n",
    "# #                 ax[i].set_title=f\"AAAAAA\"\n",
    "#                 ax[i].scatter(outlier_data.index.get_level_values(1), outlier_data[column], s=40, color=\"black\", alpha=0.7, marker=\"x\", zorder=-2)\n",
    "#             ax[-1].figure.suptitle(f\"combined noise: {num_all_outliers}, noise %:{num_all_outliers/len(input_df):.5f}%\")\n",
    "#             print(f\"norad id: {name}, rows:{len(input_df)}, combined noise count: {num_all_outliers}, noise %:{num_all_outliers/len(input_df):.5f}%\")\n",
    "#             print(f\"last 30 {all_normal[-30:].astype(int)}\")\n",
    "#             plt.show()\n",
    "        \n",
    "#         # should just return boolean mask with index from input\n",
    "#         return pd.Series(all_normal.astype(bool), index=sub_df.index)\n",
    "    \n",
    "#     # combine mask from each group then reset, sort, etc.\n",
    "#     return df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(detect_outliers).droplevel([0,2]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithreaded version of DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def detect_outliers(input_df):\n",
    "    columns = [\"INCLINATION\",\"ECCENTRICITY\",\"ARG_OF_PERICENTER\",\"RA_OF_ASC_NODE\"]\n",
    "    name = input_df.name\n",
    "    dbscan_min_samples = max(len(input_df)/100, 20)\n",
    "\n",
    "    sub_df = input_df.set_index('EPOCH', append=True).sort_index(level=1)\n",
    "    outlier_labels = []\n",
    "    for i,column in enumerate(columns):\n",
    "        if column in [\"ARG_OF_PERICENTER\",\"RA_OF_ASC_NODE\"]:\n",
    "            c_diff = sub_df[column].diff(-1)\n",
    "            c_diff_adj = np.minimum(np.abs(c_diff), 360-np.abs(c_diff))\n",
    "            c_diff = c_diff_adj * np.sign(c_diff)\n",
    "            col_diff = np.minimum(c_diff.diff()**2, c_diff.diff(-1)**2).fillna(0) + np.minimum(c_diff.diff(2)**2, c_diff.diff(-2)**2).fillna(0)\n",
    "            dbscan_eps = col_diff.std()*1\n",
    "            if not dbscan_eps > 0.0: # should never or rarely happen, but has happened before....\n",
    "                dbscan_eps = 1 # arbitary, which should mean no outliers for this satellite\n",
    "                dbscan_eps_zero_neg.append(name) # keep track of it\n",
    "\n",
    "            db = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(col_diff.to_frame())\n",
    "        else:\n",
    "            col_diff = np.minimum(sub_df[column].diff()**2, sub_df[column].diff(-1)**2).fillna(0) + np.minimum(sub_df[column].diff(2)**2, sub_df[column].diff(-2)**2).fillna(0)\n",
    "            dbscan_eps = col_diff.std()*3\n",
    "            if not dbscan_eps > 0.0: # should never or rarely happen, but has happened before....\n",
    "                dbscan_eps = 1 # arbitary, which should mean no outliers for this satellite\n",
    "                dbscan_eps_zero_neg.append(name) # keep track of it\n",
    "\n",
    "            db = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(col_diff.to_frame())\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        outlier_labels.append(labels)\n",
    "\n",
    "    all_normal = (np.array(outlier_labels).T.min(axis=1) != -1)\n",
    "\n",
    "    normal_data = sub_df[all_normal]\n",
    "\n",
    "    # should just return boolean mask with index from input\n",
    "    return pd.Series(all_normal.astype(bool), index=sub_df.index)\n",
    "\n",
    "def process_dbscan_batch(df, b):\n",
    "    tqdm.pandas(desc=f'Batch {b}',leave=False)\n",
    "    return df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(detect_outliers).droplevel([0,2]).sort_index()\n",
    "\n",
    "def dbscan_removal(df, threaded=False, batch_size=10, num_workers=5):\n",
    "    if threaded:\n",
    "        df_out = pd.DataFrame()\n",
    "        norads = df['NORAD_CAT_ID'].unique()\n",
    "        batches = [norads[i:i+batch_size] for i in range(0, len(norads), batch_size)]\n",
    "        pbar = tqdm(total=len(norads), desc='DBSCAN (threaded)')\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            tasks = []\n",
    "            for b, norad_batch in enumerate(batches):\n",
    "                t = executor.submit(process_dbscan_batch, df[df.NORAD_CAT_ID.isin(norad_batch)], b)\n",
    "                t.add_done_callback(lambda p: pbar.update(batch_size))\n",
    "                tasks.append(t)\n",
    "            for t in concurrent.futures.as_completed(tasks):\n",
    "                df_out = pd.concat([df_out, t])\n",
    "        pbar.close()\n",
    "        return df_out.droplevel([0,2]).sort_index()\n",
    "    else:\n",
    "        tqdm.pandas(desc='DBSCAN (non-threaded)')\n",
    "        return df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(detect_outliers).droplevel([0,2]).sort_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e099da781640ca9e79d52c84b13164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DBSCAN (threaded):   0%|          | 0/12764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 0:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 6:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 7:   0%|          | 0/40 [00:06<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 8:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 9:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 10:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 11:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 12:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 13:   0%|          | 0/40 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 14:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 15:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 16:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 17:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 18:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 19:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 20:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 21:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 22:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 23:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 24:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 25:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 26:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 27:   0%|          | 0/40 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 28:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 29:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 30:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 31:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0915f24a2d04a028aed3e982deb5ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 32:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0a1fe3734d419db01cca6417464586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 33:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39225e229d3461aa157d69a36e14231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 34:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a80661b7294a3dbeb51247a6fbd0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 35:   0%|          | 0/40 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a41cb0025c45e49dc2fe207ac0f8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 36:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36d9119a420408ab635a88e41a2e75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 37:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5caaba3d5445f8a26462d6191e411d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 38:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d0ed393d954a6f9291800db067ea6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 39:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbscan_eps_zero_neg = [] # this is to catch cases where std is negative(!!!?) or zero\n",
    "dbscan_mask = dbscan_removal(masked_df, threaded=True, num_workers=8, batch_size=40)\n",
    "#dbscan_mask = dbscan_removal(masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases where std is negative or zero\n",
    "# Seems to be satellites with only a single entry, safe to ignore\n",
    "\n",
    "# dbscan_eps_zero_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final filtering of minimum entry count is needed? (not really because DBSCAN's min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df[dbscan_mask].groupby(\"NORAD_CAT_ID\")['EPOCH'].count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save DataFrame with anomaly removed\n",
    "\n",
    "`min` version is saved to `2_min` in the shared data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# save both masks\n",
    "combined.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/anomaly_mask.pkl\")\n",
    "#dbscan_mask.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/dbscan_mask.pkl\")\n",
    "dbscan_mask = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/dbscan_mask.pkl\")\n",
    "\n",
    "# save min version\n",
    "masked_df[dbscan_mask].to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/train.pkl\")\n",
    "\n",
    "# We don't save full version anymore, since we no longer need the extra information\n",
    "# # load\n",
    "# train_df_full = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../1_full/train.pkl\")\n",
    "# masked_df_full = train_df_full[combined]\n",
    "# masked_df_full[dbscan_mask].to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_full/train.pkl\")\n",
    "\n",
    "# del train_df_full, masked_df_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
