{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6f7b1d",
   "metadata": {},
   "source": [
    "# Remove Abnormal Data from Training Set\n",
    "\n",
    "Load DataFrame from `0_min/train.pkl`\n",
    "\n",
    "Define multile functions that take a dataframe and return a boolean mask for entries to keep.\n",
    "\n",
    "Combine masks in the end into a final mask, then it can be applied to `0_full` version as well if that's preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c717d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4775b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('my_home_path' not in os.environ) and ('MY_HOME_PATH' in os.environ):\n",
    "    os.environ['my_home_path'] = os.environ['MY_HOME_PATH'] # because stupid :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b15252",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../0_min/train.pkl\")\n",
    "\n",
    "# total unique NORAD_IDs\n",
    "# len(train_df.NORAD_CAT_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acdbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(train_df.NORAD_CAT_ID.unique(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d802f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using a small subset for testing\n",
    "# sample_ids = [12223, 26285, 10760, 14345, 34588, 330, 20970]\n",
    "# sample_ids = [12223, 26285, 10760, 14345, 34588, 330, 20970, 35253, 38899, 36390, 27507, 31539,  8386,  6299, 18428, 17228, 42126]\n",
    "# sample_ids += list(np.random.choice(train_df.NORAD_CAT_ID.unique(),20))\n",
    "# sample_df = train_df[train_df.NORAD_CAT_ID.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e6295",
   "metadata": {},
   "source": [
    "## Remove Erroneous Data\n",
    "First we start by removing data which we don't want to include in our model.  This includes values which are outside of acceptable ranges or are physically impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512cd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early TLEs are more prone to errors, cut off should be somewhere in the 80s\n",
    "# TODO: putting in 1990 for now to be on the safe side\n",
    "\n",
    "def more_recent_only(df):\n",
    "    mask = df.EPOCH > \"1990\"\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a0b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space track LEO definition: Mean Motion > 11.25 and Eccentricity < 0.25\n",
    "# This means that satellite that decay into LEO will not have non-LEO-like entries removed\n",
    "\n",
    "def leo_check(df):\n",
    "    mask = (df['MEAN_MOTION'] > 11.25) & (df['ECCENTRICITY'] < 0.25)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0320cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid range for 'RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY' is 0..360\n",
    "\n",
    "def degrees_range_check(df):\n",
    "    degree_columns = ['RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY']\n",
    "    mask = df[degree_columns].apply(lambda x:x.between(0,360), axis=0).all(axis=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39628ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid INCLINATION range is 0..180\n",
    "\n",
    "def inclination_range_check(df):\n",
    "    mask = df['INCLINATION'].between(0,180)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4816b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Anything beyond 20 should be outliers... I think...\n",
    "# > 16.5 you do get multiple entries from the same satellite, so those shouldn't be outliers\n",
    "# train_df[train_df['MEAN_MOTION'] > 16.5].NORAD_CAT_ID.value_counts()    \n",
    "\n",
    "def mean_motion_range_check(df):\n",
    "    mask = df['MEAN_MOTION'].between(11.25,20)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6332e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping first few entries may be a good idea due to initial readings being less accurate (?)\n",
    "# TODO: using N=5 for now\n",
    "# this one takes longer because of the grouping\n",
    "\n",
    "def skip_first_n(df, n=5):\n",
    "    mask = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).apply(lambda x:x.EPOCH.rank() > n).reset_index(level=0, drop=True).sort_index()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa6e867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072a4f3cf7254117a3e0572de3d936fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing function: \"more_recent_only\"\n",
      "CPU times: user 179 ms, sys: 14.4 ms, total: 194 ms\n",
      "Wall time: 193 ms\n",
      "==========================================================\n",
      "Processing function: \"leo_check\"\n",
      "CPU times: user 156 ms, sys: 43.3 ms, total: 199 ms\n",
      "Wall time: 198 ms\n",
      "==========================================================\n",
      "Processing function: \"degrees_range_check\"\n",
      "CPU times: user 722 ms, sys: 476 ms, total: 1.2 s\n",
      "Wall time: 1.2 s\n",
      "==========================================================\n",
      "Processing function: \"inclination_range_check\"\n",
      "CPU times: user 159 ms, sys: 55.4 ms, total: 215 ms\n",
      "Wall time: 213 ms\n",
      "==========================================================\n",
      "Processing function: \"mean_motion_range_check\"\n",
      "CPU times: user 147 ms, sys: 79.5 ms, total: 227 ms\n",
      "Wall time: 226 ms\n",
      "==========================================================\n",
      "Processing function: \"skip_first_n\"\n",
      "CPU times: user 50 s, sys: 5.62 s, total: 55.6 s\n",
      "Wall time: 55.6 s\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "anomaly_functions = [\n",
    "    more_recent_only,\n",
    "    leo_check,\n",
    "    degrees_range_check,\n",
    "    inclination_range_check,\n",
    "    mean_motion_range_check,\n",
    "    skip_first_n,\n",
    "]\n",
    "\n",
    "anomaly_results = []\n",
    "for fn in tqdm(anomaly_functions):\n",
    "    print(f\"Processing function: \\\"{fn.__name__}\\\"\")\n",
    "    %time res = fn(train_df)\n",
    "    print(\"==========================================================\")\n",
    "    res.name = fn.__name__\n",
    "    anomaly_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f0c10a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     50453855\n",
       "False     4785984\n",
       "Name: more_recent_only, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     54678326\n",
       "False      561513\n",
       "Name: leo_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     55239837\n",
       "False           2\n",
       "Name: degrees_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True    55239839\n",
       "Name: inclination_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     54678335\n",
       "False      561504\n",
       "Name: mean_motion_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     55167118\n",
       "False       72721\n",
       "Name: skip_first_n, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Masks combined:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     49901955\n",
       "False     5337884\n",
       "Name: combined_masks, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mask results\n",
    "\n",
    "for s in anomaly_results:\n",
    "    display(s.value_counts())\n",
    "    \n",
    "combined = pd.concat(anomaly_results, axis=1).all(axis=1)\n",
    "combined.name = \"combined_masks\"\n",
    "print(\"==========================================================\\nMasks combined:\")\n",
    "display(combined.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01035cda",
   "metadata": {},
   "source": [
    "## Generate a new DataFrame for Outliers\n",
    "\n",
    "Masked version of DataFrame for unsupervised learning outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f5c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = train_df[combined]\n",
    "# masked_sample_df = masked_df[masked_df.NORAD_CAT_ID.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf6180",
   "metadata": {},
   "source": [
    "## Use Unsupervised Learning to Remove Outliers\n",
    "\n",
    "We'll be using `DBSCAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f097f7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_eps_zero_neg = []\n",
    "\n",
    "def dbscan_removal(df, debug=False):\n",
    "#     columns = [\"INCLINATION\",\"ECCENTRICITY\",\"MEAN_MOTION\"]\n",
    "    # mean motion turns out to be not very good, due to the final decay as well as outliers reflected in other fields as well\n",
    "    columns = [\"INCLINATION\",\"ECCENTRICITY\"]\n",
    "\n",
    "    def detect_outliers(input_df):\n",
    "        name = input_df.name\n",
    "        dbscan_min_samples = max(len(input_df)/100, 20)\n",
    "        \n",
    "        sub_df = input_df.set_index('EPOCH', append=True).sort_index(level=1)\n",
    "        outlier_labels = []\n",
    "        for i,column in enumerate(columns):\n",
    "            col_diff = np.minimum(sub_df[column].diff()**2, sub_df[column].diff(-1)**2).fillna(0) + np.minimum(sub_df[column].diff(2)**2, sub_df[column].diff(-2)**2).fillna(0)\n",
    "            dbscan_eps = col_diff.std()*3\n",
    "            if not dbscan_eps > 0.0: # should never or rarely happen, but has happened before....\n",
    "                dbscan_eps = 1 # arbitary, which should mean no outliers for this satellite\n",
    "                dbscan_eps_zero_neg.append(name) # keep track of it\n",
    "            \n",
    "            db = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(col_diff.to_frame())\n",
    "            core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "            core_samples_mask[db.core_sample_indices_] = True\n",
    "            labels = db.labels_\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            outlier_labels.append(labels)\n",
    "\n",
    "        all_normal = (np.array(outlier_labels).T.min(axis=1) != -1)\n",
    "\n",
    "        normal_data = sub_df[all_normal]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"=============================\\nnorad id: {name}, rows:{len(input_df)}\")\n",
    "            ax = (sub_df[columns].droplevel(0)).plot(subplots=True,figsize=(20,6));\n",
    "            outlier_data = sub_df[~all_normal]\n",
    "            num_all_outliers = len(input_df)-np.sum(all_normal)\n",
    "            for i,column in enumerate(columns):\n",
    "                labels = outlier_labels[i]\n",
    "                n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                n_noise_ = list(labels).count(-1)    \n",
    "                print(f\"column: {column}, n_clusters_: {n_clusters_}, n_noise_: {n_noise_}, noise %:{n_noise_/len(input_df):.5f}%\")\n",
    "\n",
    "                ax[i].scatter(outlier_data.index.get_level_values(1), outlier_data[column], s=40, color=\"red\", alpha=1, marker=\"x\", zorder=-1)\n",
    "    #             ax[i].set_title=f\"{column} #clusters: {n_clusters_}, #noise: {n_noise_}, noise %:{n_noise_/len(input_df):.5f}%\"\n",
    "                ax[i].set_title=f\"AAAAAA\"\n",
    "                ax[i].scatter(outlier_data.index.get_level_values(1), outlier_data[column], s=40, color=\"black\", alpha=0.7, marker=\"x\", zorder=-2)\n",
    "            ax[-1].figure.suptitle(f\"combined noise: {num_all_outliers}, noise %:{num_all_outliers/len(input_df):.5f}%\")\n",
    "            print(f\"norad id: {name}, rows:{len(input_df)}, combined noise count: {num_all_outliers}, noise %:{num_all_outliers/len(input_df):.5f}%\")\n",
    "            print(f\"last 30 {all_normal[-30:].astype(int)}\")\n",
    "            plt.show()\n",
    "        \n",
    "        # should just return boolean mask with index from input\n",
    "        return pd.Series(all_normal.astype(bool), index=sub_df.index)\n",
    "    \n",
    "    # combine mask from each group then reset, sort, etc.\n",
    "    return df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(detect_outliers).droplevel([0,2]).sort_index()\n",
    "    \n",
    "# dbscan_removal(masked_sample_df[(masked_sample_df.EPOCH > \"1991-9\") & (masked_sample_df.EPOCH < \"1992\") & (masked_sample_df.NORAD_CAT_ID == 12223)])\n",
    "# dbscan_removal(masked_sample_df[(masked_sample_df.NORAD_CAT_ID == 330)])\n",
    "# dbscan_mask = dbscan_removal(masked_sample_df[(masked_sample_df.NORAD_CAT_ID.isin([20970, 26285]))], True)\n",
    "# dbscan_mask = dbscan_removal(masked_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3eff83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a628424fb93487a97d764dc87511f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbscan_mask = dbscan_removal(masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8225e1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5843,\n",
       " 5843,\n",
       " 7848,\n",
       " 7848,\n",
       " 12192,\n",
       " 12192,\n",
       " 13189,\n",
       " 13189,\n",
       " 13313,\n",
       " 13313,\n",
       " 13358,\n",
       " 13358,\n",
       " 13404,\n",
       " 13404,\n",
       " 13620,\n",
       " 13620,\n",
       " 15606,\n",
       " 15606,\n",
       " 15692,\n",
       " 15692,\n",
       " 15703,\n",
       " 15703,\n",
       " 15717,\n",
       " 15717,\n",
       " 15737,\n",
       " 15737,\n",
       " 16038,\n",
       " 16038,\n",
       " 16042,\n",
       " 16042,\n",
       " 16073,\n",
       " 16073,\n",
       " 16078,\n",
       " 16078,\n",
       " 16151,\n",
       " 16151,\n",
       " 16155,\n",
       " 16155,\n",
       " 16280,\n",
       " 16280,\n",
       " 16346,\n",
       " 16346,\n",
       " 16347,\n",
       " 16347,\n",
       " 16413,\n",
       " 16413,\n",
       " 16534,\n",
       " 16534,\n",
       " 16565,\n",
       " 16565,\n",
       " 16566,\n",
       " 16566,\n",
       " 16663,\n",
       " 16663,\n",
       " 16988,\n",
       " 16988,\n",
       " 17060,\n",
       " 17060,\n",
       " 17186,\n",
       " 17186,\n",
       " 17221,\n",
       " 17221,\n",
       " 17313,\n",
       " 17313,\n",
       " 17384,\n",
       " 17384,\n",
       " 17454,\n",
       " 17454,\n",
       " 17539,\n",
       " 17539,\n",
       " 17545,\n",
       " 17545,\n",
       " 17546,\n",
       " 17546,\n",
       " 17547,\n",
       " 17547,\n",
       " 17549,\n",
       " 17549,\n",
       " 17604,\n",
       " 17604,\n",
       " 17606,\n",
       " 17606,\n",
       " 17607,\n",
       " 17607,\n",
       " 17645,\n",
       " 17645,\n",
       " 17657,\n",
       " 17657,\n",
       " 17671,\n",
       " 17671,\n",
       " 17685,\n",
       " 17685,\n",
       " 17700,\n",
       " 17700,\n",
       " 17740,\n",
       " 17740,\n",
       " 17745,\n",
       " 17745,\n",
       " 17751,\n",
       " 17751,\n",
       " 17774,\n",
       " 17774,\n",
       " 17818,\n",
       " 17818,\n",
       " 17993,\n",
       " 17993,\n",
       " 18167,\n",
       " 18167,\n",
       " 18248,\n",
       " 18248,\n",
       " 18289,\n",
       " 18289,\n",
       " 18295,\n",
       " 18295,\n",
       " 18425,\n",
       " 18425,\n",
       " 18600,\n",
       " 18600,\n",
       " 28488,\n",
       " 29085,\n",
       " 29085,\n",
       " 29140,\n",
       " 29140,\n",
       " 31352,\n",
       " 31352,\n",
       " 33641,\n",
       " 33641,\n",
       " 35907,\n",
       " 35907,\n",
       " 37480,\n",
       " 37480,\n",
       " 37700,\n",
       " 37700,\n",
       " 37910,\n",
       " 37910,\n",
       " 38126,\n",
       " 38126,\n",
       " 39809,\n",
       " 39809,\n",
       " 39831,\n",
       " 39831,\n",
       " 39894,\n",
       " 39894,\n",
       " 39909,\n",
       " 39909,\n",
       " 40643,\n",
       " 40643,\n",
       " 40682,\n",
       " 40682,\n",
       " 40767,\n",
       " 40767,\n",
       " 40791,\n",
       " 40791,\n",
       " 40809,\n",
       " 40809,\n",
       " 42078,\n",
       " 42078,\n",
       " 42105,\n",
       " 42105,\n",
       " 42108,\n",
       " 42163,\n",
       " 42163,\n",
       " 42438,\n",
       " 42438,\n",
       " 42519,\n",
       " 42519,\n",
       " 42600,\n",
       " 42600,\n",
       " 43331,\n",
       " 43331,\n",
       " 44439,\n",
       " 44439,\n",
       " 44913,\n",
       " 44913,\n",
       " 45001,\n",
       " 45001,\n",
       " 45011,\n",
       " 45011,\n",
       " 45014,\n",
       " 45014,\n",
       " 45032,\n",
       " 45032,\n",
       " 45129,\n",
       " 45129,\n",
       " 45329,\n",
       " 45329,\n",
       " 45354,\n",
       " 45354,\n",
       " 45928,\n",
       " 45928,\n",
       " 45946,\n",
       " 45946,\n",
       " 45965,\n",
       " 45965,\n",
       " 46240,\n",
       " 46240,\n",
       " 46405,\n",
       " 46405,\n",
       " 46420,\n",
       " 46420,\n",
       " 46623,\n",
       " 46623,\n",
       " 46633,\n",
       " 46633]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# might be interesting, cases where std is negative(!!!?) or zero\n",
    "dbscan_eps_zero_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7339cb",
   "metadata": {},
   "source": [
    "# Save DataFrame with anomaly removed\n",
    "\n",
    "Both `full` and `min` version is saved to `2_full` and `2_min` in the shared data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1b3d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 36.9 s, total: 1min 46s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# save both masks\n",
    "combined.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/anomaly_mask.pkl\")\n",
    "dbscan_mask.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/dbscan_mask.pkl\")\n",
    "\n",
    "# save min version\n",
    "masked_df[dbscan_mask].to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/train.pkl\")\n",
    "\n",
    "# load\n",
    "train_df_full = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../0_full/train.pkl\")\n",
    "masked_df_full = train_df_full[combined]\n",
    "masked_df_full[dbscan_mask].to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_full/train.pkl\")\n",
    "\n",
    "del train_df_full, masked_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283dc72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
