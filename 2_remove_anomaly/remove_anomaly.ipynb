{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Abnormal Data from Training Set\n",
    "\n",
    "Load DataFrame from `1_min/train.pkl`\n",
    "\n",
    "Define multile functions that take a dataframe and return a boolean mask for entries to keep.\n",
    "\n",
    "Combine masks in the end into a final mask, then it can be applied to `1_full` version as well if that's preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('my_home_path' not in os.environ) and ('MY_HOME_PATH' in os.environ):\n",
    "    os.environ['my_home_path'] = os.environ['MY_HOME_PATH'] # because stupid :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../1_min/train.pkl\")\n",
    "\n",
    "# total unique NORAD_IDs\n",
    "# len(train_df.NORAD_CAT_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.choice(train_df.NORAD_CAT_ID.unique(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just using a small subset for testing\n",
    "# sample_ids = [12223, 26285, 10760, 14345, 34588, 330, 20970]\n",
    "# sample_ids = [12223, 26285, 10760, 14345, 34588, 330, 20970, 35253, 38899, 36390, 27507, 31539,  8386,  6299, 18428, 17228, 42126]\n",
    "# sample_ids += list(np.random.choice(train_df.NORAD_CAT_ID.unique(),20))\n",
    "# sample_df = train_df[train_df.NORAD_CAT_ID.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Erroneous Data\n",
    "First we start by removing data which we don't want to include in our model.  This includes values which are outside of acceptable ranges or are physically impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early TLEs are more prone to errors, cut off should be somewhere in the 80s\n",
    "# TODO: putting in 1990 for now to be on the safe side\n",
    "\n",
    "def more_recent_only(df):\n",
    "    mask = df.EPOCH > \"1990\"\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space track LEO definition: Mean Motion > 11.25 and Eccentricity < 0.25\n",
    "# This means that satellite that decay into LEO will not have non-LEO-like entries removed\n",
    "\n",
    "def leo_check(df):\n",
    "    mask = (df['MEAN_MOTION'] > 11.25) & (df['ECCENTRICITY'] < 0.25)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid range for 'RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY' is 0..360\n",
    "\n",
    "def degrees_range_check(df):\n",
    "    degree_columns = ['RA_OF_ASC_NODE', 'ARG_OF_PERICENTER', 'MEAN_ANOMALY']\n",
    "    mask = df[degree_columns].apply(lambda x:x.between(0,360), axis=0).all(axis=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid INCLINATION range is 0..180\n",
    "\n",
    "def inclination_range_check(df):\n",
    "    mask = df['INCLINATION'].between(0,180)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Anything beyond 20 should be outliers... I think...\n",
    "# > 16.5 you do get multiple entries from the same satellite, so those shouldn't be outliers\n",
    "# train_df[train_df['MEAN_MOTION'] > 16.5].NORAD_CAT_ID.value_counts()    \n",
    "\n",
    "def mean_motion_range_check(df):\n",
    "    mask = df['MEAN_MOTION'].between(11.25,20)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping first few entries may be a good idea due to initial readings being less accurate (?)\n",
    "# TODO: using N=5 for now\n",
    "# this one takes longer because of the grouping\n",
    "\n",
    "def skip_first_n(df, n=5):\n",
    "    mask = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).apply(lambda x:x.EPOCH.rank() > n).reset_index(level=0, drop=True).sort_index()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832e255a2ccd494185347dd7a834f123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing function: \"more_recent_only\"\n",
      "CPU times: user 177 ms, sys: 37 ms, total: 214 ms\n",
      "Wall time: 211 ms\n",
      "==========================================================\n",
      "Processing function: \"leo_check\"\n",
      "CPU times: user 163 ms, sys: 96.4 ms, total: 260 ms\n",
      "Wall time: 258 ms\n",
      "==========================================================\n",
      "Processing function: \"degrees_range_check\"\n",
      "CPU times: user 961 ms, sys: 895 ms, total: 1.86 s\n",
      "Wall time: 1.85 s\n",
      "==========================================================\n",
      "Processing function: \"inclination_range_check\"\n",
      "CPU times: user 178 ms, sys: 79.9 ms, total: 258 ms\n",
      "Wall time: 256 ms\n",
      "==========================================================\n",
      "Processing function: \"mean_motion_range_check\"\n",
      "CPU times: user 176 ms, sys: 82.7 ms, total: 258 ms\n",
      "Wall time: 257 ms\n",
      "==========================================================\n",
      "Processing function: \"skip_first_n\"\n",
      "CPU times: user 49.3 s, sys: 8.27 s, total: 57.6 s\n",
      "Wall time: 57.6 s\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "anomaly_functions = [\n",
    "    more_recent_only,\n",
    "    leo_check,\n",
    "    degrees_range_check,\n",
    "    inclination_range_check,\n",
    "    mean_motion_range_check,\n",
    "    skip_first_n,\n",
    "]\n",
    "\n",
    "anomaly_results = []\n",
    "for fn in tqdm(anomaly_functions):\n",
    "    print(f\"Processing function: \\\"{fn.__name__}\\\"\")\n",
    "    %time res = fn(train_df)\n",
    "    print(\"==========================================================\")\n",
    "    res.name = fn.__name__\n",
    "    anomaly_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     50453855\n",
       "False     4785984\n",
       "Name: more_recent_only, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     54678326\n",
       "False      561513\n",
       "Name: leo_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     55239837\n",
       "False           2\n",
       "Name: degrees_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True    55239839\n",
       "Name: inclination_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     54678335\n",
       "False      561504\n",
       "Name: mean_motion_range_check, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     55167118\n",
       "False       72721\n",
       "Name: skip_first_n, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Masks combined:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True     49901955\n",
       "False     5337884\n",
       "Name: combined_masks, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mask results\n",
    "\n",
    "for s in anomaly_results:\n",
    "    display(s.value_counts())\n",
    "    \n",
    "combined = pd.concat(anomaly_results, axis=1).all(axis=1)\n",
    "combined.name = \"combined_masks\"\n",
    "print(\"==========================================================\\nMasks combined:\")\n",
    "display(combined.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a new DataFrame for Outliers\n",
    "\n",
    "Masked version of DataFrame for unsupervised learning outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_df = train_df[combined]\n",
    "# masked_sample_df = masked_df[masked_df.NORAD_CAT_ID.isin(sample_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Unsupervised Learning to Remove Outliers\n",
    "\n",
    "We'll be using `DBSCAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def dbscan_removal(df, debug=False):\n",
    "#     columns = [\"INCLINATION\",\"ECCENTRICITY\",\"MEAN_MOTION\"]\n",
    "    # mean motion turns out to be not very good, due to the final decay as well as outliers reflected in other fields as well\n",
    "    columns = [\"INCLINATION\",\"ECCENTRICITY\"]\n",
    "\n",
    "    def detect_outliers(input_df):\n",
    "        name = input_df.name\n",
    "        dbscan_min_samples = max(len(input_df)/100, 20)\n",
    "        \n",
    "        sub_df = input_df.set_index('EPOCH', append=True).sort_index(level=1)\n",
    "        outlier_labels = []\n",
    "        for i,column in enumerate(columns):\n",
    "            col_diff = np.minimum(sub_df[column].diff()**2, sub_df[column].diff(-1)**2).fillna(0) + np.minimum(sub_df[column].diff(2)**2, sub_df[column].diff(-2)**2).fillna(0)\n",
    "            dbscan_eps = col_diff.std()*3\n",
    "            if not dbscan_eps > 0.0: # should never or rarely happen, but has happened before....\n",
    "                dbscan_eps = 1 # arbitary, which should mean no outliers for this satellite\n",
    "                dbscan_eps_zero_neg.append(name) # keep track of it\n",
    "            \n",
    "            db = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(col_diff.to_frame())\n",
    "            core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "            core_samples_mask[db.core_sample_indices_] = True\n",
    "            labels = db.labels_\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            outlier_labels.append(labels)\n",
    "\n",
    "        all_normal = (np.array(outlier_labels).T.min(axis=1) != -1)\n",
    "\n",
    "        normal_data = sub_df[all_normal]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"=============================\\nnorad id: {name}, rows:{len(input_df)}\")\n",
    "            ax = (sub_df[columns].droplevel(0)).plot(subplots=True,figsize=(20,6));\n",
    "            outlier_data = sub_df[~all_normal]\n",
    "            num_all_outliers = len(input_df)-np.sum(all_normal)\n",
    "            for i,column in enumerate(columns):\n",
    "                labels = outlier_labels[i]\n",
    "                n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                n_noise_ = list(labels).count(-1)    \n",
    "                print(f\"column: {column}, n_clusters_: {n_clusters_}, n_noise_: {n_noise_}, noise %:{n_noise_/len(input_df):.5f}%\")\n",
    "\n",
    "                ax[i].scatter(outlier_data.index.get_level_values(1), outlier_data[column], s=40, color=\"red\", alpha=1, marker=\"x\", zorder=-1)\n",
    "    #             ax[i].set_title=f\"{column} #clusters: {n_clusters_}, #noise: {n_noise_}, noise %:{n_noise_/len(input_df):.5f}%\"\n",
    "#                 ax[i].set_title=f\"AAAAAA\"\n",
    "                ax[i].scatter(outlier_data.index.get_level_values(1), outlier_data[column], s=40, color=\"black\", alpha=0.7, marker=\"x\", zorder=-2)\n",
    "            ax[-1].figure.suptitle(f\"combined noise: {num_all_outliers}, noise %:{num_all_outliers/len(input_df):.5f}%\")\n",
    "            print(f\"norad id: {name}, rows:{len(input_df)}, combined noise count: {num_all_outliers}, noise %:{num_all_outliers/len(input_df):.5f}%\")\n",
    "            print(f\"last 30 {all_normal[-30:].astype(int)}\")\n",
    "            plt.show()\n",
    "        \n",
    "        # should just return boolean mask with index from input\n",
    "        return pd.Series(all_normal.astype(bool), index=sub_df.index)\n",
    "    \n",
    "    # combine mask from each group then reset, sort, etc.\n",
    "    return df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(detect_outliers).droplevel([0,2]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1dab9b1f18438fb0c21eb860c66847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbscan_eps_zero_neg = [] # this is to catch cases where std is negative(!!!?) or zero\n",
    "dbscan_mask = dbscan_removal(masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases where std is negative or zero\n",
    "# Seems to be satellites with only a single entry, safe to ignore\n",
    "\n",
    "# dbscan_eps_zero_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final filtering of minimum entry count is needed? (not really because DBSCAN's min_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12298.000000\n",
       "mean      4045.699057\n",
       "std       4611.819140\n",
       "min         19.000000\n",
       "25%        509.000000\n",
       "50%       1923.000000\n",
       "75%       6351.750000\n",
       "max      20898.000000\n",
       "Name: EPOCH, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_df[dbscan_mask].groupby(\"NORAD_CAT_ID\")['EPOCH'].count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save DataFrame with anomaly removed\n",
    "\n",
    "`min` version is saved to `2_min` in the shared data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 s, sys: 6.06 s, total: 8.2 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# save both masks\n",
    "combined.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/anomaly_mask.pkl\")\n",
    "dbscan_mask.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/dbscan_mask.pkl\")\n",
    "\n",
    "# save min version\n",
    "masked_df[dbscan_mask].to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_min/train.pkl\")\n",
    "\n",
    "# We don't save full version anymore, since we no longer need the extra information\n",
    "# # load\n",
    "# train_df_full = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../1_full/train.pkl\")\n",
    "# masked_df_full = train_df_full[combined]\n",
    "# masked_df_full[dbscan_mask].to_pickle(f\"{os.environ['GP_HIST_PATH']}/../2_full/train.pkl\")\n",
    "\n",
    "# del train_df_full, masked_df_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
