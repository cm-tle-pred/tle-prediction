{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import normalize_data\n",
    "import random\n",
    "import train\n",
    "\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this model, the data preprocessing part is already completed with the exception of scaling.\n",
    "# so we just need to scale here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_X_y(df):\n",
    "    ref_cols = [c for c in df.columns if c.startswith('__')]\n",
    "    X_cols = [c for c in df.columns if c.startswith('X_')]\n",
    "    y_cols = [c for c in df.columns if c.startswith('y_')]\n",
    "    return (df[ref_cols], df[X_cols], df[y_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {} # loads raw data and stores as a dict cache\n",
    "\n",
    "def dataset_key(dataset='', validation=False):\n",
    "    return dataset+('test' if validation else 'train')\n",
    "\n",
    "\n",
    "def load_data(raw, dataset='', validation=False):\n",
    "    '''\n",
    "    Return dataframe matching data set and validation. Dictionary input will be updated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : dict\n",
    "        dictionary which caches the dataframes and will be updated accordingly\n",
    "\n",
    "    dataset : str\n",
    "        which dataset to use? valid input includes: empty str for full set, sample_, and secret_\n",
    "\n",
    "    validation : bool\n",
    "        load validation set? if true then use _test, otherwise use _train.  Note secret_ doesn't have _train\n",
    "    '''\n",
    "    key = dataset+('test' if validation else 'train')\n",
    "    if key not in raw:\n",
    "        print(f\"Loading data to cache for: {key}\")\n",
    "        raw[key] = pd.read_pickle(f'{os.environ[\"GP_HIST_PATH\"]}/../t1_data/{key}.pkl')\n",
    "    return raw[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    'dataset' : 'sample_', # '', 'sample_', 'secret_'\n",
    "    'model_identifier' : \"sample_4\",\n",
    "    'model_path' : f\"{os.environ['GP_HIST_PATH']}/../t1_models\",\n",
    "    'device' : 'cpu',\n",
    "    'random_seed' : 0,\n",
    "    'lr' : 1e-3,\n",
    "    'momentum' : 0.9,\n",
    "    'weight_decay' : 1e-4,\n",
    "    'max_epochs' : 100,\n",
    "    'do_validate' : True,\n",
    "    'model_definition' : {\n",
    "        'layer0' : 30, 'relu0' : True, 'drop0' : 0.1,\n",
    "        'layer1' : 30, 'relu1' : True, 'drop1' : 0.1,\n",
    "        'layer2' : 30, 'relu2' : True, 'drop2' : 0.2,\n",
    "        'layer3' : 30, 'relu3' : True, 'drop3' : 0.2,\n",
    "        'layer4' : 30, 'relu4' : True, 'drop4' : 0.3,\n",
    "        'layer5' : 30, 'relu5' : True, 'drop5' : 0.3,\n",
    "        'layer6' : 30, 'relu6' : True, 'drop6' : 0.4,\n",
    "        'layer7' : 30, 'relu7' : True, 'drop7' : 0.4,\n",
    "        'layer8' : 30, 'relu8' : True, 'drop8' : 0.5,\n",
    "        'layer9' : 30, 'relu9' : True, 'drop9' : 0.5,\n",
    "        'layer10': 30, 'relu10': True, 'drop10': 0.5,\n",
    "        'layer11': 30, 'relu11': True, 'drop11': 0.5,\n",
    "        'layer12': 30, 'relu12': True, 'drop12': 0.5,\n",
    "        'layer13': 30, 'relu13': True, 'drop13': 0.5,\n",
    "        'layer14': 30, 'relu14': True, 'drop14': 0.5,\n",
    "        'layer15': 30, 'relu15': True, 'drop15': 0.5,\n",
    "        'layer16': 30, 'relu16': True, 'drop16': 0.5,\n",
    "        'layer17': 30, 'relu17': True, 'drop17': 0.5,\n",
    "        'layer18': 30, 'relu18': True, 'drop18': 0.5,\n",
    "        'layer19': 30, 'relu19': True, 'drop19': 0.5,\n",
    "    },\n",
    "    'train_params' : {\n",
    "        'batch_size': 2000,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "    'test_params' : {\n",
    "        'batch_size': 20000,\n",
    "        'num_workers': 2,\n",
    "        'pin_memory': True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to cache for: sample_train\n",
      "Loading data to cache for: sample_test\n",
      "CPU times: user 6.93 s, sys: 8.63 s, total: 15.6 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = normalize_data.normalize_all_columns(load_data(raw_data,dataset=configurations['dataset'],validation=False))\n",
    "test_df = normalize_data.normalize_all_columns(load_data(raw_data,dataset=configurations['dataset'],validation=True))\n",
    "\n",
    "ref_train, X_train, y_train = get_ref_X_y(train_df)\n",
    "ref_test, X_test, y_test = get_ref_X_y(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model created\n",
      "NNModelEx(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=29, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (16): ReLU()\n",
      "    (17): Dropout(p=0.3, inplace=False)\n",
      "    (18): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (19): ReLU()\n",
      "    (20): Dropout(p=0.4, inplace=False)\n",
      "    (21): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (22): ReLU()\n",
      "    (23): Dropout(p=0.4, inplace=False)\n",
      "    (24): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): Dropout(p=0.5, inplace=False)\n",
      "    (27): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (28): ReLU()\n",
      "    (29): Dropout(p=0.5, inplace=False)\n",
      "    (30): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (31): ReLU()\n",
      "    (32): Dropout(p=0.5, inplace=False)\n",
      "    (33): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (34): ReLU()\n",
      "    (35): Dropout(p=0.5, inplace=False)\n",
      "    (36): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (37): ReLU()\n",
      "    (38): Dropout(p=0.5, inplace=False)\n",
      "    (39): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (40): ReLU()\n",
      "    (41): Dropout(p=0.5, inplace=False)\n",
      "    (42): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (43): ReLU()\n",
      "    (44): Dropout(p=0.5, inplace=False)\n",
      "    (45): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (46): ReLU()\n",
      "    (47): Dropout(p=0.5, inplace=False)\n",
      "    (48): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (49): ReLU()\n",
      "    (50): Dropout(p=0.5, inplace=False)\n",
      "    (51): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (52): ReLU()\n",
      "    (53): Dropout(p=0.5, inplace=False)\n",
      "    (54): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (55): ReLU()\n",
      "    (56): Dropout(p=0.5, inplace=False)\n",
      "    (57): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (58): ReLU()\n",
      "    (59): Dropout(p=0.5, inplace=False)\n",
      "    (60): Linear(in_features=30, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93488da3e32463582c7378ed80e1bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd3106099e54804a5ce91403e237413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, mean_losses = train.train_model(X_train, y_train, X_test, y_test, configurations, force_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, mean_losses, _ = train.load_model_with_config(configurations)\n",
    "\n",
    "tl, vl = zip(*mean_losses)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(tl, label=\"Training Loss\")\n",
    "ax.plot(vl, label=\"Validation Loss\")\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred = train.predict(trained_model, X_train[cols], y_train, device=device) # get predictions for each train\n",
    "# y_train_pred_df = pd.DataFrame(y_train_pred, columns=y_train.columns)  # put results into a dataframe\n",
    "# # y_test_pred = train.predict(trained_model, X_test[cols], y_test, device=device) # get predictions for each train\n",
    "# # y_test_pred_df = pd.DataFrame(y_test_pred, columns=y_test.columns)  # put results into a dataframe\n",
    "\n",
    "# # print(f'    Test set MAE (L1) loss: {mean_absolute_error(y_test, y_test_pred_df)}')\n",
    "# # print(f'    Test set MSE (L2) loss: {mean_squared_error(y_test, y_test_pred_df)}')\n",
    "\n",
    "# random.seed(0)\n",
    "# #sample = random.sample(list(y_train_pred_df.index), 5)\n",
    "# sample = [0,1]\n",
    "\n",
    "# print(\"Train - Ground Truth (normalized):\")\n",
    "# display(y_train.loc[sample])\n",
    "# print(\"Train - Ground Truth (non-normalized):\")\n",
    "# display(clean_data.normalize_all_columns(y_train.loc[sample].copy(), reverse=True))  # see ground truths\n",
    "# print(\"Train - Prediction (normalized):\")\n",
    "# display(y_train_pred_df.loc[sample])\n",
    "# print(\"Train - Prediction (non-normalized):\")\n",
    "# display(clean_data.normalize_all_columns(y_train_pred_df.loc[sample].copy(), reverse=True))  # See predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def row_to_compare(X, y, y_pred, row):\n",
    "#     epoch = X.iloc[row].EPOCH_y\n",
    "#     X0 = clean_data.normalize_all_columns(X_train.iloc[row].copy(), reverse=True)\n",
    "#     y0 = clean_data.normalize_all_columns(y_train.iloc[row].copy(), reverse=True)\n",
    "#     y1 = clean_data.normalize_all_columns(y_train_pred_df.iloc[row].copy(), reverse=True)\n",
    "\n",
    "#     # Ground truth\n",
    "#     y0_xyz = clean_data.get_satellite_xyz(bst=0.0001,\n",
    "#                                           ecc=y0.ECCENTRICITY,\n",
    "#                                           aop=y0.ARG_OF_PERICENTER,\n",
    "#                                           inc=y0.INCLINATION,\n",
    "#                                           mea=y0.MEAN_ANOMALY,\n",
    "#                                           mem=y0.MEAN_MOTION,\n",
    "#                                           raa=y0.RA_OF_ASC_NODE,\n",
    "#                                           epoch=epoch,)\n",
    "#     # Prediction\n",
    "#     y1_xyz = clean_data.get_satellite_xyz(bst=0.0001,\n",
    "#                                           ecc=y1.ECCENTRICITY,\n",
    "#                                           aop=y1.ARG_OF_PERICENTER,\n",
    "#                                           inc=y1.INCLINATION,\n",
    "#                                           mea=y1.MEAN_ANOMALY,\n",
    "#                                           mem=y1.MEAN_MOTION,\n",
    "#                                           raa=y1.RA_OF_ASC_NODE,\n",
    "#                                           epoch=epoch,)\n",
    "#     # Propigation\n",
    "#     y2_xyz = clean_data.get_satellite_xyz(bst=X0.BSTAR,\n",
    "#                                           ecc=X0.ECCENTRICITY,\n",
    "#                                           aop=X0.ARG_OF_PERICENTER,\n",
    "#                                           inc=X0.INCLINATION,\n",
    "#                                           mea=X0.MEAN_ANOMALY,\n",
    "#                                           mem=X0.MEAN_MOTION,\n",
    "#                                           raa=X0.RA_OF_ASC_NODE,\n",
    "#                                           epoch=epoch,)\n",
    "#     print(f'Ground Truth: {y0_xyz}')\n",
    "#     print(f'Predicted: {y1_xyz}')\n",
    "#     print(f'Propigation: {y2_xyz}')\n",
    "    \n",
    "#     print (f'Prediction Error: {sum((y1_xyz-y0_xyz)**2)**0.5} km')\n",
    "#     print (f'Propigation Error: {sum((y2_xyz-y0_xyz)**2)**0.5} km')\n",
    "    \n",
    "\n",
    "# for row in range(2):\n",
    "#     print (f'Row {row}:')\n",
    "#     row_to_compare(X_train, y_train, y_train_pred_df, row)\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
