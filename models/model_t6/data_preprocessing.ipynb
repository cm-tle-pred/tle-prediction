{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sgp4.api import Satrec, SatrecArray, WGS72\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global dataset\n",
    "dataset = \"test\" # variable for lazy loading defaultdict\n",
    "input_files = [\n",
    "    \"train\",\n",
    "    \"test\",\n",
    "    \"secret_test\",\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "# using defaultdict to lazy load dataframes.... probably should stay in notebook as shortcut only\n",
    "data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}.pkl\"))\n",
    "tle_sup_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../tle_sup/{dataset}.pkl\"))\n",
    "sgp4_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}_sgp4rv.pkl\"))\n",
    "# satrec_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}_satrec.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __jday_convert(x):\n",
    "    '''\n",
    "    Algorithm from python-sgp4:\n",
    "\n",
    "    from sgp4.functions import jday\n",
    "    jday(x.year, x.month, x.day, x.hour, x.minute, x.second + x.microsecond * 1e-6)\n",
    "    '''\n",
    "    jd = (367.0 * x.year\n",
    "         - 7 * (x.year + ((x.month + 9) // 12.0)) * 0.25 // 1.0\n",
    "           + 275 * x.month / 9.0 // 1.0\n",
    "           + x.day\n",
    "         + 1721013.5)\n",
    "    fr = (x.second + (x.microsecond * 1e-6) + x.minute * 60.0 + x.hour * 3600.0) / 86400.0;\n",
    "    return jd, fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_repr(s,v):\n",
    "    cos = np.cos(np.deg2rad(s * (360/v)))\n",
    "    sin = np.sin(np.deg2rad(s * (360/v)))\n",
    "    return cos,sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_values(df):\n",
    "    name = df.name\n",
    "    df = df.sort_values(\"EPOCH\")\n",
    "    \n",
    "    df[\"SUBGROUP\"] = 0\n",
    "    \n",
    "    doycos, doysin = cyclic_repr(df.EPOCH.dt.dayofyear, 366)\n",
    "    df[\"DAY_OF_YEAR_COS\"] = doycos\n",
    "    df[\"DAY_OF_YEAR_SIN\"] = doysin\n",
    "    \n",
    "    macos, masin = cyclic_repr(df.MEAN_ANOMALY, 360)\n",
    "    df[\"MEAN_ANOMALY_COS\"] = macos\n",
    "    df[\"MEAN_ANOMALY_SIN\"] = masin\n",
    "    \n",
    "    icos, isin = cyclic_repr(df.INCLINATION, 360)\n",
    "    df[\"INCLINATION_COS\"] = icos\n",
    "    df[\"INCLINATION_SIN\"] = isin\n",
    "    \n",
    "    rcos, rsin = cyclic_repr(df.RA_OF_ASC_NODE, 360)\n",
    "    df[\"RA_OF_ASC_NODE_COS\"] = rcos\n",
    "    df[\"RA_OF_ASC_NODE_SIN\"] = rsin\n",
    "    \n",
    "    df[['EPOCH_JD', 'EPOCH_FR']] = df.EPOCH.apply(__jday_convert).to_list()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is 1 groupby of satellite\n",
    "def generate_X_y(df):\n",
    "    idx = df.name\n",
    "\n",
    "    df = df.reset_index(level=1).drop_duplicates(subset=['EPOCH']).sort_values(\"EPOCH\")\n",
    "    dfs = []\n",
    "    for i in range(0,20):\n",
    "        dfi = pd.concat([df.add_suffix(\"_1\"),df.shift(-i).add_suffix(\"_2\")], axis=1).dropna()\n",
    "        dfs.append(dfi)\n",
    "    ddf = pd.concat(dfs).reset_index(drop=True)\n",
    "    # Reference variables only, DO NOT USE TO TRAIN\n",
    "    __cols = [\n",
    "        'NORAD_CAT_ID_1','GP_ID_1','GP_ID_2','EPOCH_1','EPOCH_2',\n",
    "    ]\n",
    "    df = ddf[__cols]\n",
    "    df.columns = ['__'+x for x in __cols]\n",
    "    \n",
    "    # X\n",
    "    x_cols = [\n",
    "        'EPOCH_JD_1', 'EPOCH_FR_1', 'EPOCH_JD_2', 'EPOCH_FR_2',\n",
    "        'MEAN_MOTION_DOT_1', 'BSTAR_1', 'INCLINATION_1', 'RA_OF_ASC_NODE_1', 'ECCENTRICITY_1', 'ARG_OF_PERICENTER_1',\n",
    "        'MEAN_ANOMALY_1', 'MEAN_MOTION_1',\n",
    "        'MEAN_ANOMALY_COS_1', 'MEAN_ANOMALY_SIN_1',\n",
    "        'INCLINATION_COS_1', 'INCLINATION_SIN_1',\n",
    "        'RA_OF_ASC_NODE_COS_1', 'RA_OF_ASC_NODE_SIN_1',\n",
    "        'SEMIMAJOR_AXIS_1', 'PERIOD_1', 'APOAPSIS_1', 'PERIAPSIS_1', 'RCS_SIZE_1',\n",
    "        'SAT_RX_1', 'SAT_RY_1', 'SAT_RZ_1', 'SAT_VX_1', 'SAT_VY_1', 'SAT_VZ_1',\n",
    "        'YEAR_1', 'DAY_OF_YEAR_COS_1', 'DAY_OF_YEAR_SIN_1',\n",
    "        'SUNSPOTS_1D_1', 'SUNSPOTS_3D_1', 'SUNSPOTS_7D_1',\n",
    "        'AIR_MONTH_AVG_TEMP_1','WATER_MONTH_AVG_TEMP_1',\n",
    "    ]\n",
    "    \n",
    "    df['X_delta_EPOCH'] = (ddf.EPOCH_2 - ddf.EPOCH_1).astype(int) / 86400000000000 # in days\n",
    "    df[['X_'+x for x in x_cols]] = ddf[x_cols]\n",
    "\n",
    "    y_cols = ['SAT_RX', 'SAT_RY', 'SAT_RZ', 'SAT_VX', 'SAT_VY', 'SAT_VZ']\n",
    "    df[['y_'+y for y in y_cols]] = ddf[[y+'_2' for y in y_cols]]\n",
    "    df['y_SAT_R'] = np.sqrt((df[['y_SAT_RX', 'y_SAT_RY', 'y_SAT_RZ']]**2).sum(axis=1))\n",
    "    df['y_SAT_V'] = np.sqrt((df[['y_SAT_VX', 'y_SAT_VY', 'y_SAT_VZ']]**2).sum(axis=1))\n",
    "\n",
    "    df = df[(df['X_delta_EPOCH'] < 14) & (df['X_delta_EPOCH'] > 0.04)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_thing(df, f):\n",
    "    df = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "    df = df.merge(sgp4_data[f], left_index=True, right_index=True)\n",
    "    df = df.merge(tle_sup_data[f], left_on=\"GP_ID\", right_index=True)\n",
    "    df = df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179f277a2216434e9b0f28f5437fbbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c3d571c39241f2a4234c7acbe01c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154171374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ee28aea8bc418da8a5f3d9a03f846f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a6a0a628c447bb9a54965ddcce8ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11496647\n"
     ]
    }
   ],
   "source": [
    "# generate smaller set from training set to test\n",
    "prefix = \"sample3_\"\n",
    "\n",
    "dataset = \"train\"\n",
    "train_ids = np.random.choice(data[dataset].NORAD_CAT_ID.unique(), 3000)\n",
    "sample_train_df = data[dataset][data[dataset].NORAD_CAT_ID.isin(train_ids)]\n",
    "sample_train_df = do_the_thing(sample_train_df, dataset)\n",
    "sample_train_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t6_data/{prefix}train.pkl\")\n",
    "print(len(sample_train_df))\n",
    "\n",
    "dataset = \"test\"\n",
    "test_ids = np.random.choice(data[dataset].NORAD_CAT_ID.unique(), 200)\n",
    "sample_test_df = data[dataset][data[dataset].NORAD_CAT_ID.isin(test_ids)]\n",
    "sample_test_df = do_the_thing(sample_test_df, dataset)\n",
    "sample_test_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t6_data/{prefix}test.pkl\")\n",
    "print(len(sample_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ab4a72ce9b40df88ac92d62c008875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c23c0cf48144525b73520f02ad3a8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in input_files:\n",
    "    dataset = f # variable for lazy loading defaultdict\n",
    "    print(f\"Preparing data for: {f}\")\n",
    "    df = do_the_thing(data[f], dataset)\n",
    "    df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t6_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset = \"test\" # set the lazy loader\n",
    "# sample_df = data[dataset][data[dataset].NORAD_CAT_ID.isin([20885, 7128])]#, 4756\n",
    "# sample_df = sample_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# # sample_df = sample_df.merge(sgp4_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.merge(tle_sup_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# sample_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"train\" # variable for lazy loading defaultdict\n",
    "# print(f\"Preparing data for: {f}\")\n",
    "# df = data[f].groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# df = df.merge(sgp4_data[f], left_index=True, right_index=True)\n",
    "# df = df.merge(tle_sup_data[f], left_on=\"GP_ID\", right_index=True)\n",
    "# df = df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t6_data/{f}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
