{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sgp4.api import Satrec, SatrecArray, WGS72\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global dataset\n",
    "dataset = \"test\" # variable for lazy loading defaultdict\n",
    "input_files = [\n",
    "    \"train\",\n",
    "    \"test\",\n",
    "    \"secret_test\",\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "# using defaultdict to lazy load dataframes.... probably should stay in notebook as shortcut only\n",
    "data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}.pkl\"))\n",
    "tle_sup_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../tle_sup/{dataset}.pkl\"))\n",
    "sgp4_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}_sgp4rv.pkl\"))\n",
    "# satrec_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}_satrec.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __jday_convert(x):\n",
    "    '''\n",
    "    Algorithm from python-sgp4:\n",
    "\n",
    "    from sgp4.functions import jday\n",
    "    jday(x.year, x.month, x.day, x.hour, x.minute, x.second + x.microsecond * 1e-6)\n",
    "    '''\n",
    "    jd = (367.0 * x.year\n",
    "         - 7 * (x.year + ((x.month + 9) // 12.0)) * 0.25 // 1.0\n",
    "           + 275 * x.month / 9.0 // 1.0\n",
    "           + x.day\n",
    "         + 1721013.5)\n",
    "    fr = (x.second + (x.microsecond * 1e-6) + x.minute * 60.0 + x.hour * 3600.0) / 86400.0;\n",
    "    return jd, fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_repr(s,v):\n",
    "    cos = np.cos(np.deg2rad(s * (360/v)))\n",
    "    sin = np.sin(np.deg2rad(s * (360/v)))\n",
    "    return cos,sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_values(df):\n",
    "    name = df.name\n",
    "    df = df.sort_values(\"EPOCH\")\n",
    "    \n",
    "    # convert ARG_OF_PERICENTER, RA_OF_ASC_NODE, and MEAN_ANOMALY to non-cyclic version\n",
    "    df[\"ARG_OF_PERICENTER_ADJUSTED\"] = np.cumsum(np.around(df.ARG_OF_PERICENTER.diff().fillna(0) / -360))*360 + df.ARG_OF_PERICENTER\n",
    "    df[\"RA_OF_ASC_NODE_ADJUSTED\"] = np.cumsum(np.around(df.RA_OF_ASC_NODE.diff().fillna(0) / -360))*360 + df.RA_OF_ASC_NODE\n",
    "    \n",
    "    # according to 18 SPCS there was only 1 such case BUT ITS NOT TRUE there are like 70+\n",
    "    # this is because for REV_AT_EPOCH = 100,000, it's recorded as 10,000 instead of 0\n",
    "    # this doesn't handle the case for multiple ground stations reporting though, if the previous is different....\n",
    "    # would it be better to just remove this as an outlier just to be safe?\n",
    "    # 90k +- 20 max offset based on MEAN_MOTION maximum from earlier steps\n",
    "    df.loc[(df.REV_AT_EPOCH==10000) & df.REV_AT_EPOCH.diff().between(-89999,-89940),'REV_AT_EPOCH'] = 0\n",
    "\n",
    "    # combine REV_AT_EPOCH and MEAN_ANOMALY for a non-cyclic representation\n",
    "    adjusted_rev = df.REV_AT_EPOCH + np.cumsum(np.around(df.REV_AT_EPOCH.diff().fillna(0) / -100000)) * 100000\n",
    "    df[\"REV_MEAN_ANOMALY_COMBINED\"] = adjusted_rev * 360 + df.MEAN_ANOMALY\n",
    "    \n",
    "    # this is to handle the REV_AT_EPOCH problem inconsistency problem\n",
    "    # otherwise the REV_MEAN_ANOMALY_COMBINED difference may be incorrect\n",
    "    # bfill because we may start at non-zero due to previous data removal bit\n",
    "    a = np.round((adjusted_rev.diff().fillna(method='bfill')/300)).fillna(0)\n",
    "    df[\"SUBGROUP\"] = np.cumsum(a).astype(int)\n",
    "    \n",
    "    doycos, doysin = cyclic_repr(df.EPOCH.dt.dayofyear, 366)\n",
    "    df[\"DAY_OF_YEAR_COS\"] = doycos\n",
    "    df[\"DAY_OF_YEAR_SIN\"] = doysin\n",
    "    \n",
    "    macos, masin = cyclic_repr(df.MEAN_ANOMALY, 360)\n",
    "    df[\"MEAN_ANOMALY_COS\"] = macos\n",
    "    df[\"MEAN_ANOMALY_SIN\"] = masin\n",
    "    \n",
    "    icos, isin = cyclic_repr(df.INCLINATION, 360)\n",
    "    df[\"INCLINATION_COS\"] = icos\n",
    "    df[\"INCLINATION_SIN\"] = isin\n",
    "    \n",
    "    rcos, rsin = cyclic_repr(df.RA_OF_ASC_NODE, 360)\n",
    "    df[\"RA_OF_ASC_NODE_COS\"] = rcos\n",
    "    df[\"RA_OF_ASC_NODE_SIN\"] = rsin\n",
    "    \n",
    "    df[['EPOCH_JD', 'EPOCH_FR']] = df.EPOCH.apply(__jday_convert).to_list()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is 1 groupby of satellite\n",
    "def generate_X_y(df):\n",
    "    idx = df.name\n",
    "\n",
    "    df = df.reset_index(level=1).drop_duplicates(subset=['EPOCH']).sort_values(\"EPOCH\")\n",
    "    dfs = []\n",
    "    for i in [1,2,3,4,6,8,11,14]:\n",
    "        dfi = pd.concat([df.add_suffix(\"_1\"),df.shift(-i).add_suffix(\"_2\")], axis=1).dropna()\n",
    "        dfs.append(dfi)\n",
    "    ddf = pd.concat(dfs).reset_index(drop=True)\n",
    "    # Reference variables only, DO NOT USE TO TRAIN\n",
    "    __cols = [\n",
    "        'NORAD_CAT_ID_1','GP_ID_1','GP_ID_2','EPOCH_1','EPOCH_2',\n",
    "#         'SAT_RX_2', 'SAT_RY_2', 'SAT_RZ_2', 'SAT_VX_2', 'SAT_VY_2', 'SAT_VZ_2', # these are ground truths\n",
    "    ]\n",
    "    df = ddf[__cols]\n",
    "    df.columns = ['__'+x for x in __cols]\n",
    "    \n",
    "    # X\n",
    "    x_cols = [\n",
    "        'EPOCH_JD_1', 'EPOCH_FR_1', 'EPOCH_JD_2', 'EPOCH_FR_2',\n",
    "        'MEAN_MOTION_DOT_1', 'BSTAR_1', 'INCLINATION_1', 'RA_OF_ASC_NODE_1', 'ECCENTRICITY_1', 'ARG_OF_PERICENTER_1',\n",
    "        'MEAN_ANOMALY_1', 'MEAN_MOTION_1',\n",
    "        'MEAN_ANOMALY_COS_1', 'MEAN_ANOMALY_SIN_1',\n",
    "        'INCLINATION_COS_1', 'INCLINATION_SIN_1',\n",
    "        'RA_OF_ASC_NODE_COS_1', 'RA_OF_ASC_NODE_SIN_1',\n",
    "        'SEMIMAJOR_AXIS_1', 'PERIOD_1', 'APOAPSIS_1', 'PERIAPSIS_1', 'RCS_SIZE_1',\n",
    "        'SAT_RX_1', 'SAT_RY_1', 'SAT_RZ_1', 'SAT_VX_1', 'SAT_VY_1', 'SAT_VZ_1',\n",
    "        'YEAR_1', 'DAY_OF_YEAR_COS_1', 'DAY_OF_YEAR_SIN_1',\n",
    "        'SUNSPOTS_1D_1', 'SUNSPOTS_3D_1', 'SUNSPOTS_7D_1',\n",
    "        'AIR_MONTH_AVG_TEMP_1','WATER_MONTH_AVG_TEMP_1',\n",
    "    ]\n",
    "    \n",
    "    df['X_delta_EPOCH'] = (ddf.EPOCH_2 - ddf.EPOCH_1).astype(int) / 86400000000000 # in days\n",
    "    df[['X_'+x for x in x_cols]] = ddf[x_cols]\n",
    "\n",
    "    # not sure if this day limiting thing makes sense....\n",
    "    # add 'X_SGP4_SAT_RX', 'X_SGP4_SAT_RY', 'X_SGP4_SAT_RZ', 'X_SGP4_SAT_VX', 'X_SGP4_SAT_VY', 'X_SGP4_SAT_VZ'\n",
    "#     df = df.groupby(by=[\"__GP_ID_1\"]).apply(add_sgp4_propagation)\n",
    "\n",
    "    y_cols = ['BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE', 'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_MOTION', 'MEAN_ANOMALY']\n",
    "    df[['y_'+y for y in y_cols]] = ddf[[y+'_2' for y in y_cols]]\n",
    "    \n",
    "    df['y_REV_MA_REG'] = ((ddf.REV_MEAN_ANOMALY_COMBINED_2 - ddf.REV_MEAN_ANOMALY_COMBINED_1) + ddf.MEAN_ANOMALY_1) / 360\n",
    "    df['y_ARG_OF_PERICENTER_REG'] = (ddf.ARG_OF_PERICENTER_ADJUSTED_2 - ddf.ARG_OF_PERICENTER_ADJUSTED_1 + ddf.ARG_OF_PERICENTER_1) / 360\n",
    "    df['y_RA_OF_ASC_NODE_REG'] = (ddf.RA_OF_ASC_NODE_ADJUSTED_2 - ddf.RA_OF_ASC_NODE_ADJUSTED_1 + ddf.RA_OF_ASC_NODE_1) / 360\n",
    "\n",
    "    df = df[(df['X_delta_EPOCH'] < 7) & (df['X_delta_EPOCH'] > 0.1) & ((df.y_REV_MA_REG / df.X_delta_EPOCH).between(df.X_MEAN_MOTION_1*0.99,df.X_MEAN_MOTION_1*1.01))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_thing(df, f):\n",
    "    df = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "    df = df.merge(sgp4_data[f], left_index=True, right_index=True)\n",
    "    df = df.merge(tle_sup_data[f], left_on=\"GP_ID\", right_index=True)\n",
    "    df = df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e024fffdd0141319e16067466b4aac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51109b2294cc458ab94dc003198a4355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1466 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35289f780af47908eef39b6dde7dd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e508767a8a24c81a67d386a64c7aa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583944\n",
      "77051\n"
     ]
    }
   ],
   "source": [
    "# generate smaller set from training set to test\n",
    "\n",
    "dataset = \"train\"\n",
    "\n",
    "# narrow down further with random norad IDs\n",
    "train_ids = np.random.choice(data[dataset].NORAD_CAT_ID.unique(), 100)\n",
    "test_ids = np.random.choice(list(set(data[dataset].NORAD_CAT_ID.unique())-set(train_ids)),10)\n",
    "\n",
    "sample_train_df = data[dataset][data[dataset].NORAD_CAT_ID.isin(train_ids)]\n",
    "sample_test_df = data[dataset][data[dataset].NORAD_CAT_ID.isin(test_ids)]\n",
    "\n",
    "sample_train_df = do_the_thing(sample_train_df, dataset)\n",
    "sample_test_df = do_the_thing(sample_test_df, dataset)\n",
    "\n",
    "# save samples\n",
    "sample_train_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t4_data/sample_train.pkl\")\n",
    "sample_test_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t4_data/sample_test.pkl\")\n",
    "\n",
    "print(len(sample_train_df))\n",
    "print(len(sample_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f64c2bd9f646a1a74437bcc16e48d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf475666b9b431aa3acd207bfb6a9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a7fddd06ff4059aaba23729fdaa1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc77feb9c88e4fceb13a63f5b7581e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: secret_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d799a699d548eb9db27b6a809e6d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a875caa339984ec8bdd138d3703e12a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in input_files:\n",
    "    dataset = f # variable for lazy loading defaultdict\n",
    "    print(f\"Preparing data for: {f}\")\n",
    "    df = do_the_thing(data[f], dataset)\n",
    "    df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t4_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset = \"test\" # set the lazy loader\n",
    "# sample_df = data[dataset][data[dataset].NORAD_CAT_ID.isin([20885, 7128])]#, 4756\n",
    "# sample_df = sample_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# # sample_df = sample_df.merge(sgp4_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.merge(tle_sup_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# sample_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"train\" # variable for lazy loading defaultdict\n",
    "# print(f\"Preparing data for: {f}\")\n",
    "# df = data[f].groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# df = df.merge(sgp4_data[f], left_index=True, right_index=True)\n",
    "# df = df.merge(tle_sup_data[f], left_on=\"GP_ID\", right_index=True)\n",
    "# df = df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t4_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_sgp4_propagation(df):\n",
    "#     satrec = get_satrec(bst=df.iloc[0][\"X_BSTAR_1\"],\n",
    "#                         ecc=df.iloc[0][\"X_ECCENTRICITY_1\"],\n",
    "#                         aop=df.iloc[0][\"X_ARG_OF_PERICENTER_1\"],\n",
    "#                         inc=df.iloc[0][\"X_INCLINATION_1\"],\n",
    "#                         mea=df.iloc[0][\"X_MEAN_ANOMALY_1\"],\n",
    "#                         mem=df.iloc[0][\"X_MEAN_MOTION_1\"],\n",
    "#                         raa=df.iloc[0][\"X_RA_OF_ASC_NODE_1\"],\n",
    "#                         epoch=df.iloc[0][\"__EPOCH_1\"])\n",
    "# #     satrec = satrec_data[dataset].loc[satrec_data[dataset].index == df.name,\"SATREC_OBJ\"].values[0] # this isn't any faster\n",
    "#     jd = df.X_EPOCH_JD_2.values\n",
    "#     fr = df.X_EPOCH_FR_2.values\n",
    "#     e,r,v = satrec.sgp4_array(jd,fr) # these are propagated\n",
    "#     df[['X_SGP4_SAT_RX', 'X_SGP4_SAT_RY', 'X_SGP4_SAT_RZ']] = r\n",
    "#     df[['X_SGP4_SAT_VX', 'X_SGP4_SAT_VY', 'X_SGP4_SAT_VZ']] = v\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_satrec(bst, ecc, aop, inc, mea, mem, raa, mmdot=0, mmddot=0, norad=0, epoch=None):\n",
    "#     r = datetime.strptime('12/31/1949 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
    "#     epoch_days = (epoch-r)/np.timedelta64(1, 'D')\n",
    "#     s = Satrec()\n",
    "#     s.sgp4init(\n",
    "#          WGS72,           # gravity model\n",
    "#          'i',             # 'a' = old AFSPC mode, 'i' = improved mode\n",
    "#          norad,               # satnum: Satellite number\n",
    "#          epoch_days,       # epoch: days since 1949 December 31 00:00 UT\n",
    "#          bst,      # bstar: drag coefficient (/earth radii)\n",
    "#          mmdot,   # ndot (NOT USED): ballistic coefficient (revs/day)\n",
    "#          mmddot,             # nddot (NOT USED): mean motion 2nd derivative (revs/day^3)\n",
    "#          ecc,       # ecco: eccentricity\n",
    "#          aop*np.pi/180, # argpo: argument of perigee (radians)\n",
    "#          inc*np.pi/180, # inclo: inclination (radians)\n",
    "#          mea*np.pi/180, # mo: mean anomaly (radians)\n",
    "#          mem*np.pi/(4*180), # no_kozai: mean motion (radians/minute)\n",
    "#          raa*np.pi/180, # nodeo: right ascension of ascending node (radians)\n",
    "#     )\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### old stuff below just saving it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample_df = sample_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# sample_df = sample_df.merge(sgp4_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# sample_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/train.pkl\")\n",
    "# converted_df = train_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "\n",
    "\n",
    "# # narrow down using certain inclination range only\n",
    "# # sample_df = converted_df[converted_df.INCLINATION.between(65,67)]\n",
    "# sample_df = converted_df\n",
    "\n",
    "# sgp4_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/train_sgp4rv.pkl\")\n",
    "\n",
    "# sample_df = sample_df.merge(sgp4_df, left_index=True, right_index=True)\n",
    "\n",
    "# # narrow down further with random norad IDs\n",
    "# train_ids = np.random.choice(sample_df.NORAD_CAT_ID.unique(), 200)\n",
    "# test_ids = np.random.choice(list(set(sample_df.NORAD_CAT_ID.unique())-set(train_ids)),50)\n",
    "\n",
    "# sample_train_df = sample_df[sample_df.NORAD_CAT_ID.isin(train_ids)]\n",
    "# sample_test_df = sample_df[sample_df.NORAD_CAT_ID.isin(test_ids)]\n",
    "\n",
    "# processed_sample_train_df = sample_train_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# processed_sample_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# processed_sample_test_df = sample_test_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# processed_sample_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # save samples\n",
    "# processed_sample_train_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t1_data/sample_train.pkl\")\n",
    "# processed_sample_test_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t1_data/sample_test.pkl\")\n",
    "\n",
    "# print(len(processed_sample_train_df))\n",
    "# print(len(processed_sample_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for f in input_files:\n",
    "#     print(f\"Preparing data for: {f}\")\n",
    "#     df = \n",
    "#     converted_df = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "#     sgp4_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{f}_sgp4rv.pkl\")\n",
    "#     converted_df = converted_df.merge(sgp4_df, left_index=True, right_index=True)\n",
    "#     processed_df = converted_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "#     processed_df.reset_index(drop=True, inplace=True)\n",
    "#     processed_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t1_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_feature_values(df):\n",
    "#     name = df.name\n",
    "#     df = df.sort_values(\"EPOCH\")\n",
    "    \n",
    "#     # we're skipping this part for model t1 since we aren't predicting TLEs\n",
    "# #     # convert ARG_OF_PERICENTER, RA_OF_ASC_NODE, and MEAN_ANOMALY to non-cyclic version\n",
    "# #     df[\"ARG_OF_PERICENTER_ADJUSTED\"] = np.cumsum(np.around(df.ARG_OF_PERICENTER.diff().fillna(0) / -360))*360 + df.ARG_OF_PERICENTER\n",
    "# #     df[\"RA_OF_ASC_NODE_ADJUSTED\"] = np.cumsum(np.around(df.RA_OF_ASC_NODE.diff().fillna(0) / -360))*360 + df.RA_OF_ASC_NODE\n",
    "    \n",
    "# #     # this is because for REV_AT_EPOCH = 100,000, it's recorded as 10,000 instead of 0\n",
    "# #     # this doesn't handle the case for multiple ground stations reporting though, if the previous is different....\n",
    "# #     # would it be better to just remove this as an outlier just to be safe?\n",
    "# #     # 90k +- 20 max offset based on MEAN_MOTION maximum from earlier steps\n",
    "# #     df.loc[(df.REV_AT_EPOCH==10000) & df.REV_AT_EPOCH.diff().between(-89999,-89940),'REV_AT_EPOCH'] = 0\n",
    "\n",
    "# #     # combine REV_AT_EPOCH and MEAN_ANOMALY for a non-cyclic representation\n",
    "# #     adjusted_rev = df.REV_AT_EPOCH + np.cumsum(np.around(df.REV_AT_EPOCH.diff().fillna(0) / -100000)) * 100000\n",
    "# #     df[\"REV_MEAN_ANOMALY_COMBINED\"] = adjusted_rev * 360 + df.MEAN_ANOMALY\n",
    "    \n",
    "# #     # this is to handle the REV_AT_EPOCH problem inconsistency problem\n",
    "# #     # otherwise the REV_MEAN_ANOMALY_COMBINED difference may be incorrect\n",
    "# #     # bfill because we may start at non-zero due to previous data removal bit\n",
    "# #     a = np.round((adjusted_rev.diff().fillna(method='bfill')/2000)).fillna(0)\n",
    "# #     df[\"SUBGROUP\"] = np.cumsum(a).astype(int)\n",
    "\n",
    "#     # keeping this to keep the rest of the code unchanged\n",
    "#     df[\"SUBGROUP\"] = 0\n",
    "    \n",
    "#     doycos, doysin = cyclic_repr(df.EPOCH.dt.dayofyear, 366)\n",
    "#     df[\"DAY_OF_YEAR_COS\"] = doycos\n",
    "#     df[\"DAY_OF_YEAR_SIN\"] = doysin\n",
    "    \n",
    "#     df[['EPOCH_JD', 'EPOCH_FR']] = df.EPOCH.apply(__jday_convert).to_list()\n",
    "\n",
    "# #     synodic = df.EPOCH.astype(int) % 2551442976000000\n",
    "# #     sidereal = df.EPOCH.astype(int) % 2360591510400000\n",
    "    \n",
    "# #     syn_m_cos, syn_m_sin = cyclic_repr(synodic, 2551442976000000)\n",
    "# #     df[\"SYNODIC_MONTH_COS\"] = syn_m_cos\n",
    "# #     df[\"SYNODIC_MONTH_SIN\"] = syn_m_sin\n",
    "\n",
    "# #     sr_m_cos, sr_m_sin = cyclic_repr(synodic, 2360591510400000)\n",
    "# #     df[\"SIDEREAL_MONTH_COS\"] = sr_m_cos\n",
    "# #     df[\"SIDEREAL_MONTH_SIN\"] = sr_m_sin\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is 1 groupby of satellite\n",
    "# def generate_X_y(df):\n",
    "#     idx = df.name\n",
    "\n",
    "#     df = df.reset_index().drop_duplicates(subset=['EPOCH']).sort_values(\"EPOCH\")[:10]\n",
    "#     dfs = []\n",
    "#     for i in range(1,11):\n",
    "#         dfi = pd.concat([df.add_suffix(\"_1\"),df.shift(-i).add_suffix(\"_2\")], axis=1).dropna()\n",
    "#         dfs.append(dfi)\n",
    "#     ddf = pd.concat(dfs)\n",
    "\n",
    "#     # Reference variables only, DO NOT USE TO TRAIN\n",
    "#     __cols = [\n",
    "#         'NORAD_CAT_ID_1','GP_ID_1','GP_ID_2','EPOCH_1','EPOCH_2',\n",
    "#         'SAT_RX_2', 'SAT_RY_2', 'SAT_RZ_2', 'SAT_VX_2', 'SAT_VY_2', 'SAT_VZ_2', # these are ground truths\n",
    "#     ]\n",
    "#     df = ddf[__cols]\n",
    "#     df.columns = ['__'+x for x in __cols]\n",
    "#     # Ignore these columns completely\n",
    "# #     'MONTH', 'DAY', # month and day should be well-represented as day_of_year\n",
    "# #     'REV_AT_EPOCH' # this one doesn't matter if we are predicting cartesian\n",
    "    \n",
    "#     # X\n",
    "#     x_cols = [\n",
    "#         'EPOCH_JD_1', 'EPOCH_FR_1', 'EPOCH_JD_2', 'EPOCH_FR_2',\n",
    "#         'MEAN_MOTION_DOT_1', 'BSTAR_1', 'INCLINATION_1', 'RA_OF_ASC_NODE_1', 'ECCENTRICITY_1', 'ARG_OF_PERICENTER_1',\n",
    "#         'MEAN_ANOMALY_1', 'MEAN_MOTION_1',\n",
    "#         'SAT_RX_1', 'SAT_RY_1', 'SAT_RZ_1', 'SAT_VX_1', 'SAT_VY_1', 'SAT_VZ_1',\n",
    "#         'YEAR_1', 'DAY_OF_YEAR_COS_1', 'DAY_OF_YEAR_SIN_1',\n",
    "# #               'SYNODIC_MONTH_COS', 'SYNODIC_MONTH_SIN', 'SIDEREAL_MONTH_COS', 'SIDEREAL_MONTH_SIN',\n",
    "#         'SUNSPOTS_1D_1', 'SUNSPOTS_3D_1', 'SUNSPOTS_7D_1',\n",
    "#         'AIR_MONTH_AVG_TEMP_1','WATER_MONTH_AVG_TEMP_1',\n",
    "#     ]\n",
    "    \n",
    "#     df['X_delta_EPOCH'] = (ddf.EPOCH_2 - ddf.EPOCH_1).astype(int) / 86400000000000 # in days\n",
    "#     df[['X_'+x for x in x_cols]] = ddf[x_cols]\n",
    "\n",
    "#     # y\n",
    "# #     y_cols = ['SAT_RX', 'SAT_RY', 'SAT_RZ', 'SAT_VX', 'SAT_VY', 'SAT_VZ'] #??????????????????\n",
    "# #     df[['y_'+y for y in y_cols]] = ddf[y_cols]\n",
    "    \n",
    "#     # not sure if this day limiting thing makes sense....\n",
    "#     df = df[(df['X_delta_EPOCH'] < 5) & (df['X_delta_EPOCH'] > 0.1)]\n",
    "    \n",
    "#     # add 'X_SGP4_SAT_RX', 'X_SGP4_SAT_RY', 'X_SGP4_SAT_RZ', 'X_SGP4_SAT_VX', 'X_SGP4_SAT_VY', 'X_SGP4_SAT_VZ'\n",
    "#     df = df.groupby(by=\"__GP_ID_1\").apply(add_sgp4_propagation)\n",
    "    \n",
    "#     df['y_SAT_RX_ERROR'] = df['__SAT_RX_2'] - df['X_SGP4_SAT_RX']\n",
    "#     df['y_SAT_RY_ERROR'] = df['__SAT_RY_2'] - df['X_SGP4_SAT_RY']\n",
    "#     df['y_SAT_RZ_ERROR'] = df['__SAT_RZ_2'] - df['X_SGP4_SAT_RZ']\n",
    "#     df['y_SAT_VX_ERROR'] = df['__SAT_VX_2'] - df['X_SGP4_SAT_VX']\n",
    "#     df['y_SAT_VY_ERROR'] = df['__SAT_VY_2'] - df['X_SGP4_SAT_VY']\n",
    "#     df['y_SAT_VZ_ERROR'] = df['__SAT_VZ_2'] - df['X_SGP4_SAT_VZ']\n",
    "    \n",
    "#     return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
