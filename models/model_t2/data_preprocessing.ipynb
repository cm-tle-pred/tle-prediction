{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sgp4.api import Satrec, SatrecArray, WGS72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __jday_convert(x):\n",
    "    '''\n",
    "    Algorithm from python-sgp4:\n",
    "\n",
    "    from sgp4.functions import jday\n",
    "    jday(x.year, x.month, x.day, x.hour, x.minute, x.second + x.microsecond * 1e-6)\n",
    "    '''\n",
    "    jd = (367.0 * x.year\n",
    "         - 7 * (x.year + ((x.month + 9) // 12.0)) * 0.25 // 1.0\n",
    "           + 275 * x.month / 9.0 // 1.0\n",
    "           + x.day\n",
    "         + 1721013.5)\n",
    "    fr = (x.second + (x.microsecond * 1e-6) + x.minute * 60.0 + x.hour * 3600.0) / 86400.0;\n",
    "    return jd, fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_repr(s,v):\n",
    "    cos = np.cos(np.deg2rad(s * (360/v)))\n",
    "    sin = np.sin(np.deg2rad(s * (360/v)))\n",
    "    return cos,sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_values(df):\n",
    "    name = df.name\n",
    "    df = df.sort_values(\"EPOCH\")\n",
    "    \n",
    "    \n",
    "    # convert ARG_OF_PERICENTER, RA_OF_ASC_NODE, and MEAN_ANOMALY to non-cyclic version\n",
    "    df[\"ARG_OF_PERICENTER_ADJUSTED\"] = np.cumsum(np.around(df.ARG_OF_PERICENTER.diff().fillna(0) / -360))*360 + df.ARG_OF_PERICENTER\n",
    "    df[\"RA_OF_ASC_NODE_ADJUSTED\"] = np.cumsum(np.around(df.RA_OF_ASC_NODE.diff().fillna(0) / -360))*360 + df.RA_OF_ASC_NODE\n",
    "    \n",
    "    # this is because for REV_AT_EPOCH = 100,000, it's recorded as 10,000 instead of 0\n",
    "    # this doesn't handle the case for multiple ground stations reporting though, if the previous is different....\n",
    "    # would it be better to just remove this as an outlier just to be safe?\n",
    "    # 90k +- 20 max offset based on MEAN_MOTION maximum from earlier steps\n",
    "    df.loc[(df.REV_AT_EPOCH==10000) & df.REV_AT_EPOCH.diff().between(-90020,-89980),'REV_AT_EPOCH'] = 0\n",
    "\n",
    "    # combine REV_AT_EPOCH and MEAN_ANOMALY for a non-cyclic representation\n",
    "    adjusted_rev = df.REV_AT_EPOCH + np.cumsum(np.around(df.REV_AT_EPOCH.diff().fillna(0) / -100000)) * 100000\n",
    "    df[\"REV_MEAN_ANOMALY_COMBINED\"] = adjusted_rev * 360 + df.MEAN_ANOMALY\n",
    "    \n",
    "    # this is to handle the REV_AT_EPOCH problem inconsistency problem\n",
    "    # otherwise the REV_MEAN_ANOMALY_COMBINED difference may be incorrect\n",
    "    # bfill because we may start at non-zero due to previous data removal bit\n",
    "    a = np.round((adjusted_rev.diff().fillna(method='bfill')/300)).fillna(0)\n",
    "    df[\"SUBGROUP\"] = np.cumsum(a).astype(int)\n",
    "    \n",
    "    doycos, doysin = cyclic_repr(df.EPOCH.dt.dayofyear, 366)\n",
    "    df[\"DAY_OF_YEAR_COS\"] = doycos\n",
    "    df[\"DAY_OF_YEAR_SIN\"] = doysin\n",
    "    \n",
    "#     synodic = df.EPOCH.astype(int) % 2551442976000000\n",
    "#     sidereal = df.EPOCH.astype(int) % 2360591510400000\n",
    "    \n",
    "#     syn_m_cos, syn_m_sin = cyclic_repr(synodic, 2551442976000000)\n",
    "#     df[\"SYNODIC_MONTH_COS\"] = syn_m_cos\n",
    "#     df[\"SYNODIC_MONTH_SIN\"] = syn_m_sin\n",
    "\n",
    "#     sr_m_cos, sr_m_sin = cyclic_repr(synodic, 2360591510400000)\n",
    "#     df[\"SIDEREAL_MONTH_COS\"] = sr_m_cos\n",
    "#     df[\"SIDEREAL_MONTH_SIN\"] = sr_m_sin\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is 1 groupby of satellite\n",
    "def generate_X_y(df):\n",
    "    idx = df.name\n",
    "\n",
    "    df = df.reset_index().drop_duplicates(subset=['EPOCH']).sort_values(\"EPOCH\")\n",
    "    dfs = []\n",
    "#     for i in range(1,11):\n",
    "    for i in [1,2,4,6]: # use less for now\n",
    "        dfi = pd.concat([df,df.shift(-i).add_suffix(\"_b\")], axis=1).dropna()\n",
    "        dfs.append(dfi)\n",
    "    ddf = pd.concat(dfs)\n",
    "\n",
    "    # Reference variables only, DO NOT USE TO TRAIN\n",
    "    df = ddf[['NORAD_CAT_ID','GP_ID','GP_ID_b','EPOCH','EPOCH_b']]\n",
    "    df.columns = ['__NORAD_CAT_ID','__GP_ID_1','__GP_ID_2','__EPOCH_1','__EPOCH_2']\n",
    "    df['__GP_ID_2'] = df['__GP_ID_2'].astype(int)\n",
    "    \n",
    "    # Ignore these columns completely\n",
    "#     'MONTH', 'DAY', # month and day should be well-represented as day_of_year\n",
    "#     'REV_AT_EPOCH' # this one doesn't matter if we are predicting cartesian\n",
    "    \n",
    "    # X\n",
    "    df['X_delta_EPOCH'] = (ddf.EPOCH_b - ddf.EPOCH).astype(int) / 86400000000000 # in days\n",
    "\n",
    "    df[['X_EPOCH_JD', 'X_EPOCH_FR']] = ddf.EPOCH.apply(__jday_convert).to_list()\n",
    "\n",
    "    x_cols = ['MEAN_MOTION_DOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE', 'ECCENTRICITY', 'ARG_OF_PERICENTER',\n",
    "              'MEAN_ANOMALY', 'MEAN_MOTION',\n",
    "              'YEAR', 'DAY_OF_YEAR_COS', 'DAY_OF_YEAR_SIN',\n",
    "#               'SYNODIC_MONTH_COS', 'SYNODIC_MONTH_SIN', 'SIDEREAL_MONTH_COS', 'SIDEREAL_MONTH_SIN',\n",
    "              'SUNSPOTS_1D', 'SUNSPOTS_3D', 'SUNSPOTS_7D',\n",
    "              'AIR_MONTH_AVG_TEMP','WATER_MONTH_AVG_TEMP',\n",
    "              'SAT_RX', 'SAT_RY', 'SAT_RZ', 'SAT_VX', 'SAT_VY', 'SAT_VZ',\n",
    "             ]\n",
    "    \n",
    "    df[['X_'+x for x in x_cols]] = ddf[x_cols]\n",
    "\n",
    "    # y\n",
    "    df[['y_REV_MA_REG']] = ((ddf.REV_MEAN_ANOMALY_COMBINED_b - ddf.REV_MEAN_ANOMALY_COMBINED) + ddf.MEAN_ANOMALY) / 360\n",
    "    df[['y_ARG_OF_PERICENTER_REG']] = (ddf.ARG_OF_PERICENTER_ADJUSTED_b - ddf.ARG_OF_PERICENTER_ADJUSTED + ddf.ARG_OF_PERICENTER) / 360\n",
    "    df[['y_RA_OF_ASC_NODE_REG']] = (ddf.RA_OF_ASC_NODE_ADJUSTED_b - ddf.RA_OF_ASC_NODE_ADJUSTED + ddf.RA_OF_ASC_NODE) / 360\n",
    "#     y_cols = ['SAT_RX', 'SAT_RY', 'SAT_RZ', 'SAT_VX', 'SAT_VY', 'SAT_VZ']\n",
    "#     df[['y_'+y for y in y_cols]] = ddf[y_cols]\n",
    "    \n",
    "    # not sure if this day limiting thing makes sense....\n",
    "    df = df[(df['X_delta_EPOCH'] < 5) & (df['X_delta_EPOCH'] > 0.1) & ((df.y_REV_MA_REG / df.X_delta_EPOCH).between(df.X_MEAN_MOTION*0.9,df.X_MEAN_MOTION*1.1))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07206ca40e884f59aa44cbf4a64c84ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213b51ef22f943ccbef3bacf45bfa529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87543e4613b3498f8b5c403fb38e3e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ebd0e6581c443abdbf6e9c56c3a133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: secret_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d788e520d7794e148e0301e036ba7c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92930c0c6aeb47b29e05d469927f42b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5h 15min 40s, sys: 6min 49s, total: 5h 22min 30s\n",
      "Wall time: 5h 27min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Generate actual data\n",
    "\n",
    "input_files = [\n",
    "    \"test\",\n",
    "    \"train\",\n",
    "    \"secret_test\",\n",
    "]\n",
    "\n",
    "for f in input_files:\n",
    "    print(f\"Preparing data for: {f}\")\n",
    "    df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{f}.pkl\")\n",
    "    converted_df = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "    sgp4_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{f}_sgp4rv.pkl\")\n",
    "    converted_df = converted_df.merge(sgp4_df, left_index=True, right_index=True)\n",
    "    processed_df = converted_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "    processed_df.reset_index(drop=True, inplace=True)\n",
    "    processed_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t2_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f61cf5dda148c0932634f96f457410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d3545aece245b992f5c8884e50d945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260999\n",
      "561651\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # generate smaller set from training set to test\n",
    "\n",
    "train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/train.pkl\")\n",
    "converted_df = train_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "\n",
    "\n",
    "# narrow down using certain inclination range only\n",
    "# sample_df = converted_df[converted_df.INCLINATION.between(65,67)]\n",
    "sample_df = converted_df\n",
    "\n",
    "sgp4_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/train_sgp4rv.pkl\")\n",
    "sample_df = sample_df.merge(sgp4_df, left_index=True, right_index=True)\n",
    "\n",
    "# narrow down further with random norad IDs\n",
    "train_ids = np.random.choice(sample_df.NORAD_CAT_ID.unique(), 200)\n",
    "test_ids = np.random.choice(list(set(sample_df.NORAD_CAT_ID.unique())-set(train_ids)),50)\n",
    "\n",
    "sample_train_df = sample_df[sample_df.NORAD_CAT_ID.isin(train_ids)]\n",
    "sample_test_df = sample_df[sample_df.NORAD_CAT_ID.isin(test_ids)]\n",
    "\n",
    "processed_sample_train_df = sample_train_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "processed_sample_train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "processed_sample_test_df = sample_test_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "processed_sample_test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save samples\n",
    "processed_sample_train_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t2_data/sample_train.pkl\")\n",
    "processed_sample_test_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t2_data/sample_test.pkl\")\n",
    "\n",
    "print(len(processed_sample_train_df))\n",
    "print(len(processed_sample_test_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
