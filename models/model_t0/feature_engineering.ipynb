{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8351f4c2",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Convert data into more ML friendly formats.  Reversible so the model output later can be reverted back to TLE style format.\n",
    "\n",
    "This conversion needs to be performed on all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8537fb",
   "metadata": {},
   "source": [
    "Features:\n",
    "\n",
    "| Column        | Desc  | Effect on SGP4 |\n",
    "| :------------- | :------| :----|\n",
    "| `NORAD_CAT_ID` | Satellite identifier, not used in training, no action needed |\n",
    "| `OBJECT_TYPE` | Satellite meta data, not used in training, no action needed (only in `full` version) |\n",
    "| `TLE_LINE1` | Actual TLE line 1, not used in training, no action needed (only in `full` version) |\n",
    "| `TLE_LINE2` | Actual TLE line 2, not used in training, no action needed (only in `full` version) |\n",
    "| `MEAN_MOTION_DOT` | Some sort of scaling may be needed | NOT used in SGP4 propagation |\n",
    "| `MEAN_MOTION_DDOT` | Some sort of scaling may be needed | NOT used in SGP4 propagation |\n",
    "| `BSTAR` | Some sort of scaling may be needed | Affects `v` component based on `r` (assuming higher bstar = higher drag = more decay) |\n",
    "| `INCLINATION` | Convert cyclic 0 .. 180 | Defines path of possible `r` values (0 = equator, 90 = polar orbit) |\n",
    "| `RA_OF_ASC_NODE` | Convert cyclic 0 .. 360 | Defines path of possible `r` values (kind of like rotating the orbit viewed from the poles?) |\n",
    "| `ECCENTRICITY` | Some scaling needed, 0 .. 0.25 | Defines path of possible `r` values (0 = circular orbit) |\n",
    "| `ARG_OF_PERICENTER` | Convert cyclic 0 .. 360 | Defines path of possible `r` values (0 means closest when crossing north-south reference plane) |\n",
    "| `MEAN_ANOMALY` | Convert cyclic 0 .. 360, this loops multiple times per day and most cycles are unobserved in the data | Defines which `r` position is used |\n",
    "| `MEAN_MOTION` | > 11.25 | Defines path of possible `r` values (smaller = longer orbit) |\n",
    "| `REV_AT_EPOCH` | 0-99999, but sometimes inconcsistency in data where there is an offset to this from different ground stations (a guess) | NOT used in SGP4 propagation |\n",
    "| `EPOCH` | Time, while no scaling is needed, we will need to use this for constructing `X` and `y` | Time and time offset used for propagation |\n",
    "| `GP_ID` | Unique identifier for the TLE entry, not used in training, no action needed |\n",
    "\n",
    "While `MEAN_ANOMALY` is represeted in degrees, because a lot of cycles are left out due to how sparse the data is, a combination of `REV_AT_EPOCH` + `MEAN_ANOMALY` may be a better representation of the features rather than using sin/cos representation.  Other conversion can be done without grouping, but due to `REV_AT_EPOCH` rolling over at 100k and inconsistency between ground stations, we might need to handle it per satellite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36163f57",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "```\n",
    "3_min/train.pkl\n",
    "3_min/test.pkl\n",
    "3_min/secret_test.pkl\n",
    "```\n",
    "\n",
    "Converting `min` versions only for now to save some memory and disk space.  Can be replaced with `full` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e66cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../model_0')\n",
    "from clean_data import __jday_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2a22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_repr(s,v):\n",
    "    cos = np.cos(np.deg2rad(s * (360/v)))\n",
    "    sin = np.sin(np.deg2rad(s * (360/v)))\n",
    "    return cos,sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a9df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_values(df):\n",
    "    name = df.name\n",
    "    df = df.sort_values(\"EPOCH\")\n",
    "    # convert ARG_OF_PERICENTER, RA_OF_ASC_NODE, and MEAN_ANOMALY to non-cyclic version\n",
    "    df[\"ARG_OF_PERICENTER_ADJUSTED\"] = np.cumsum(np.around(df.ARG_OF_PERICENTER.diff().fillna(0) / -360))*360 + df.ARG_OF_PERICENTER\n",
    "    df[\"RA_OF_ASC_NODE_ADJUSTED\"] = np.cumsum(np.around(df.RA_OF_ASC_NODE.diff().fillna(0) / -360))*360 + df.RA_OF_ASC_NODE\n",
    "    \n",
    "    # this is because for REV_AT_EPOCH = 100,000, it's recorded as 10,000 instead of 0\n",
    "    # this doesn't handle the case for multiple ground stations reporting though, if the previous is different....\n",
    "    # would it be better to just remove this as an outlier just to be safe?\n",
    "    # 90k +- 20 max offset based on MEAN_MOTION maximum from earlier steps\n",
    "    df.loc[(df.REV_AT_EPOCH==10000) & df.REV_AT_EPOCH.diff().between(-90020,-89980),'REV_AT_EPOCH'] = 0\n",
    "\n",
    "    # combine REV_AT_EPOCH and MEAN_ANOMALY for a non-cyclic representation\n",
    "    adjusted_rev = df.REV_AT_EPOCH + np.cumsum(np.around(df.REV_AT_EPOCH.diff().fillna(0) / -100000)) * 100000\n",
    "    df[\"REV_MEAN_ANOMALY_COMBINED\"] = adjusted_rev * 360 + df.MEAN_ANOMALY\n",
    "    \n",
    "    # this is to handle the REV_AT_EPOCH problem inconsistency problem\n",
    "    # otherwise the REV_MEAN_ANOMALY_COMBINED difference may be incorrect\n",
    "    # bfill because we may start at non-zero due to previous data removal bit\n",
    "    a = np.round((adjusted_rev.diff().fillna(method='bfill')/2000)).fillna(0)\n",
    "    df[\"SUBGROUP\"] = np.cumsum(a).astype(int)\n",
    "    \n",
    "    doycos, doysin = cyclic_repr(df.EPOCH.dt.dayofyear, 366)\n",
    "    df[\"DAY_OF_YEAR_COS\"] = doycos\n",
    "    df[\"DAY_OF_YEAR_SIN\"] = doysin\n",
    "    \n",
    "    synodic = df.EPOCH.astype(int) % 2551442976000000\n",
    "    sidereal = df.EPOCH.astype(int) % 2360591510400000\n",
    "    \n",
    "    syn_m_cos, syn_m_sin = cyclic_repr(synodic, 2551442976000000)\n",
    "    df[\"SYNODIC_MONTH_COS\"] = syn_m_cos\n",
    "    df[\"SYNODIC_MONTH_SIN\"] = syn_m_sin\n",
    "\n",
    "    sr_m_cos, sr_m_sin = cyclic_repr(synodic, 2360591510400000)\n",
    "    df[\"SIDEREAL_MONTH_COS\"] = sr_m_cos\n",
    "    df[\"SIDEREAL_MONTH_SIN\"] = sr_m_sin\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Leaving here for reference only, not actually used anymore\n",
    "# def revert_feature_values(df):\n",
    "#     df['REV_AT_EPOCH'] = ((df.REV_MEAN_ANOMALY_COMBINED // 360) % 100000).astype(int)\n",
    "#     df['MEAN_ANOMALY'] = df.REV_MEAN_ANOMALY_COMBINED % 360\n",
    "#     df['RA_OF_ASC_NODE'] = df.RA_OF_ASC_NODE_ADJUSTED % 360\n",
    "#     df['ARG_OF_PERICENTER'] = df.ARG_OF_PERICENTER_ADJUSTED % 360\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4fc0ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input is 1 groupby of satellite\n",
    "def generate_X_y(df):\n",
    "    idx = df.name\n",
    "\n",
    "    df = df.reset_index().drop_duplicates(subset=['EPOCH']).sort_values(\"EPOCH\")\n",
    "    dfs = []\n",
    "    for i in range(1,11):\n",
    "        dfi = pd.concat([df,df.shift(-i).add_suffix(\"_b\")], axis=1).dropna()\n",
    "        dfs.append(dfi)\n",
    "    ddf = pd.concat(dfs)\n",
    "\n",
    "    # Reference variables only, DO NOT USE TO TRAIN\n",
    "    df = ddf[['NORAD_CAT_ID','GP_ID','GP_ID_b','EPOCH','EPOCH_b']]\n",
    "    df.columns = ['__NORAD_CAT_ID','__GP_ID_1','__GP_ID_2','__EPOCH_1','__EPOCH_2']\n",
    "    df['__GP_ID_2'] = df['__GP_ID_2'].astype(int)\n",
    "    \n",
    "    # X\n",
    "    x_cols = ['MEAN_MOTION_DOT', 'BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE', 'ECCENTRICITY', 'ARG_OF_PERICENTER',\n",
    "              'MEAN_ANOMALY', 'MEAN_MOTION', 'REV_AT_EPOCH',\n",
    "              'DAY_OF_YEAR_COS', 'DAY_OF_YEAR_SIN', 'YEAR', 'MONTH', 'DAY',\n",
    "              'SYNODIC_MONTH_COS', 'SYNODIC_MONTH_SIN', 'SIDEREAL_MONTH_COS', 'SIDEREAL_MONTH_SIN',\n",
    "              'SUNSPOTS_7D', 'AIR_MONTH_AVG_TEMP','WATER_MONTH_AVG_TEMP']\n",
    "#     df[['X_EPOCH_JD', 'X_EPOCH_FR']] = ddf.EPOCH.apply(__jday_convert).to_list() # turns out this will overfit\n",
    "    df[['X_'+x for x in x_cols]] = ddf[x_cols]\n",
    "    df['X_delta_EPOCH'] = (ddf.EPOCH_b - ddf.EPOCH).astype(int) / 86400000000000 # in days\n",
    "    # y\n",
    "    df['y_delta_INCLINATION'] = ddf.INCLINATION_b - ddf.INCLINATION\n",
    "    df['y_delta_ECCENTRICITY'] = ddf.ECCENTRICITY_b - ddf.ECCENTRICITY\n",
    "    df['y_delta_MEAN_MOTION'] = ddf.MEAN_MOTION_b - ddf.MEAN_MOTION\n",
    "    df['y_delta_ARG_OF_PERICENTER'] = ddf.ARG_OF_PERICENTER_ADJUSTED_b - ddf.ARG_OF_PERICENTER_ADJUSTED\n",
    "    df['y_delta_RA_OF_ASC_NODE'] = ddf.RA_OF_ASC_NODE_ADJUSTED_b - ddf.RA_OF_ASC_NODE_ADJUSTED\n",
    "    df['y_delta_REV_MEAN_ANOMALY_COMBINED'] = ddf.REV_MEAN_ANOMALY_COMBINED_b - ddf.REV_MEAN_ANOMALY_COMBINED\n",
    "    \n",
    "    # not sure if this day limiting thing makes sense....\n",
    "    df = df[(df['X_delta_EPOCH'] < 5) & (df['X_delta_EPOCH'] > 0.25)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480888fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Generate actual data\n",
    "\n",
    "# input_files = [ \"train\", \"test\", \"secret_test\"]\n",
    "\n",
    "# for f in input_files:\n",
    "#     print(f\"Preparing data for: {f}\")\n",
    "#     df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{f}.pkl\")\n",
    "#     converted_df = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "#     processed_df = converted_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y).reset_index(drop=True)\n",
    "#     processed_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t0_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8787ca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369a70358d55455ba2bdc75ef3cf414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd97d0c631fa408f98284ab8a28cf549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/107 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f96f88336b1426b991a409cf4fdd9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845414\n",
      "294665\n",
      "CPU times: user 3min 18s, sys: 27.7 s, total: 3min 46s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# generate smaller set from training set to test\n",
    "\n",
    "train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/train.pkl\")\n",
    "converted_df = train_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "\n",
    "# narrow down using certain inclination range only\n",
    "sample_df = converted_df[converted_df.INCLINATION.between(65,67)]\n",
    "\n",
    "# narrow down further with random norad IDs\n",
    "train_ids = np.random.choice(sample_df.NORAD_CAT_ID.unique(), 90)\n",
    "test_ids = np.random.choice(list(set(sample_df.NORAD_CAT_ID.unique())-set(train_ids)),10)\n",
    "\n",
    "sample_train_df = sample_df[sample_df.NORAD_CAT_ID.isin(train_ids)]\n",
    "sample_test_df = sample_df[sample_df.NORAD_CAT_ID.isin(test_ids)]\n",
    "\n",
    "processed_sample_train_df = sample_train_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y).reset_index(drop=True)\n",
    "processed_sample_test_df = sample_test_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y).reset_index(drop=True)\n",
    "\n",
    "# save samples\n",
    "processed_sample_train_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t0_data/sample_train.pkl\")\n",
    "processed_sample_test_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t0_data/sample_test.pkl\")\n",
    "\n",
    "print(len(processed_sample_train_df))\n",
    "print(len(processed_sample_test_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
