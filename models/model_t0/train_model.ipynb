{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b8ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from math import ceil\n",
    "from statistics import mean\n",
    "\n",
    "import re\n",
    "#################################\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "#################################\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../model_0')\n",
    "from model import NNModelEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcd5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data needs to be re-generated, rerun this cell\n",
    "# this are globals\n",
    "all_data = None\n",
    "sampled_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36149e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, inp, tgt, device='cpu'):\n",
    "        'Initialization'\n",
    "        self.inp = to_device(torch.from_numpy(inp).float(), device)\n",
    "        self.tgt = to_device(torch.from_numpy(tgt).float(), device)\n",
    "        self.num_X = inp.shape[1]\n",
    "        self.num_y = tgt.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.tgt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.inp[index], self.tgt[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150d911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_X_y(df):\n",
    "    ref_cols = [c for c in df.columns if c.startswith('__')]\n",
    "    X_cols = [c for c in df.columns if c.startswith('X_')]\n",
    "    y_cols = [c for c in df.columns if c.startswith('y_')]\n",
    "    return (df[ref_cols], df[X_cols], df[y_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4eb076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_scale_data(ignore_cols=[], prefix=\"\"):\n",
    "    train_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../4_min/{prefix}train.pkl\")\n",
    "    test_df = pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../4_min/{prefix}test.pkl\")\n",
    "    \n",
    "    # move columns from X or y to ref\n",
    "    ignore_dict = {x:re.sub('^.', '_', x, 1) for x in ignore_cols}\n",
    "    train_df = train_df.rename(columns=ignore_dict)\n",
    "    test_df = test_df.rename(columns=ignore_dict)\n",
    "    \n",
    "    ref_train, X_train, y_train = get_ref_X_y(train_df)\n",
    "    ref_test, X_test, y_test = get_ref_X_y(test_df)\n",
    "\n",
    "    X_min_max_scaler = MinMaxScaler()\n",
    "    y_min_max_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = X_min_max_scaler.fit_transform(X_train)\n",
    "    y_train = y_min_max_scaler.fit_transform(y_train)\n",
    "    X_test = X_min_max_scaler.fit_transform(X_test)\n",
    "    y_test = y_min_max_scaler.fit_transform(y_test)\n",
    "    \n",
    "    return train_df, test_df, X_min_max_scaler, y_min_max_scaler, ref_train, X_train, y_train, ref_test, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb5cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_config(config):\n",
    "    # kinda lazy here so using global to avoid reloading the datasets\n",
    "    global sampled_data, all_data\n",
    "    if config['use_sampled'] == True:\n",
    "        if sampled_data == None:\n",
    "            print(\"Loading sampled data\")\n",
    "            sampled_data = load_and_scale_data(prefix = \"sample_\")\n",
    "        return sampled_data\n",
    "    else:\n",
    "        if all_data == None:\n",
    "            print(\"Loading all data\")\n",
    "            all_data = load_and_scale_data()\n",
    "        return all_data\n",
    "    \n",
    "def load_model_with_config(config):\n",
    "    # a bit hacky, but in the training phase, we never load and use the minmax scalers\n",
    "    # just putting it here for when we want to load the model elsewhere THEN revert scaling\n",
    "    # probably better to have the scalers saved separately....\n",
    "\n",
    "    f = f\"{os.environ['GP_HIST_PATH']}/../t_models/{config['model_identifier']}.pth\"\n",
    "    if os.path.exists(f):\n",
    "        print(\"Loading existing model\")\n",
    "        checkpoint = torch.load(f)\n",
    "        net = checkpoint['net']\n",
    "        loss_func = checkpoint['loss_func']\n",
    "        optimizer = checkpoint['optimizer']\n",
    "        mean_losses = checkpoint['mean_losses']\n",
    "        all_losses = checkpoint['all_losses']\n",
    "        next_epoch = checkpoint['next_epoch']\n",
    "        X_min_max_scaler = checkpoint['X_min_max_scaler']\n",
    "        y_min_max_scaler = checkpoint['y_min_max_scaler']\n",
    "    else:\n",
    "        print(\"New model created\")\n",
    "        net = NNModelEx(inputSize=training_set.num_X, outputSize=training_set.num_y, **config['model_definition'])\n",
    "        loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "        mean_losses = []\n",
    "        all_losses = []\n",
    "        next_epoch = 0\n",
    "        save_model_with_config(config, net=net, loss_func=loss_func, optimizer=optimizer,\n",
    "                               mean_losses=mean_losses, all_losses=all_losses, next_epoch=next_epoch,\n",
    "                               X_min_max_scaler=None, y_min_max_scaler=None,\n",
    "                              )\n",
    "        # blank scaler when creating new model\n",
    "        X_min_max_scaler = None\n",
    "        y_min_max_scaler = None\n",
    "    return net, loss_func, optimizer, mean_losses, all_losses, next_epoch, X_min_max_scaler, y_min_max_scaler\n",
    "\n",
    "def save_model_with_config(config, **kwargs):\n",
    "    f = f\"{os.environ['GP_HIST_PATH']}/../t_models/{config['model_identifier']}.pth\"\n",
    "    torch.save(kwargs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ca9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLED DATA\n",
    "configurations = {\n",
    "    'model_identifier' : \"sample_5\",\n",
    "    'use_sampled' : True,\n",
    "    'random_seed' : 0,\n",
    "    'lr' : 1e-5,\n",
    "    'weight_decay' : 1e-6,\n",
    "    'max_epochs' : 100,\n",
    "    'do_validate' : True,\n",
    "    'model_definition' : {\n",
    "        'drop__0': 0.6,\n",
    "        'layer_1': 100, 'relu__1': True, 'drop__1': 0.7,\n",
    "        'layer_2': 100, 'relu__2': True, 'drop__2': 0.7,\n",
    "        'layer_3': 100, 'relu__3': True, 'drop__3': 0.7,\n",
    "        'layer_4': 50,  'relu__4': True, 'drop__4': 0.7,\n",
    "        'layer_5': 50,  'relu__5': True, 'drop__5': 0.4,\n",
    "        'layer_6': 50,  'relu__6': True, 'drop__6': 0.4,\n",
    "        'layer_7': 50,  'relu__7': True, 'drop__7': 0.4,\n",
    "#         'layer_8': 50,  'relu__8': True, 'drop__8': 0.4,\n",
    "    },\n",
    "    'train_params' : {\n",
    "        'batch_size': 1000,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2,\n",
    "    },\n",
    "    'test_params' : {\n",
    "        'batch_size': 20000,\n",
    "        'num_workers': 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# # ALL DATA\n",
    "# configurations = {\n",
    "#     'model_identifier' : \"full_dataset_2\",\n",
    "#     'use_sampled' : False,\n",
    "#     'random_seed' : 0,\n",
    "#     'lr' : 1e-5,\n",
    "#     'weight_decay' : 1e-6,\n",
    "#     'max_epochs' : 5,\n",
    "#     'do_validate' : True,\n",
    "#     'model_definition' : {\n",
    "#         'drop__0': 0.6,\n",
    "#         'layer_1': 100, 'relu__1': True, 'drop__1': 0.5,\n",
    "#         'layer_2': 100, 'relu__2': True, 'drop__2': 0.5,\n",
    "#         'layer_3': 100, 'relu__3': True, 'drop__3': 0.7,\n",
    "#         'layer_4': 50,  'relu__4': True, 'drop__4': 0.4,\n",
    "#         'layer_5': 50,  'relu__5': True, 'drop__5': 0.4,\n",
    "#     },\n",
    "#     'train_params' : {\n",
    "#         'batch_size': 10000,\n",
    "#         'shuffle': True,\n",
    "#         'num_workers': 6,\n",
    "#     },\n",
    "#     'test_params' : {\n",
    "#         'batch_size': 200000,\n",
    "#         'num_workers': 6,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec05fe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sampled data\n",
      "Creating Dataset and Dataloader\n",
      "New model created\n",
      "NNModelEx(\n",
      "  (net): Sequential(\n",
      "    (0): Dropout(p=0.6, inplace=False)\n",
      "    (1): Linear(in_features=22, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.7, inplace=False)\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.7, inplace=False)\n",
      "    (7): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.7, inplace=False)\n",
      "    (10): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Dropout(p=0.7, inplace=False)\n",
      "    (13): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.4, inplace=False)\n",
      "    (16): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): Dropout(p=0.4, inplace=False)\n",
      "    (19): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (20): ReLU()\n",
      "    (21): Dropout(p=0.4, inplace=False)\n",
      "    (22): Linear(in_features=50, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "This model has prevoiusly been trained for 0 of 100 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e76e51981804b7083dbc1823c59ad8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64613a4c02b48bdb357ab28dc3fdb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data and base config\n",
    "train_df, test_df, X_min_max_scaler, y_min_max_scaler, \\\n",
    " ref_train, X_train, y_train, ref_test, X_test, y_test = load_data_with_config(configurations)\n",
    "print(\"Creating Dataset and Dataloader\")\n",
    "training_set = Dataset(X_train, y_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **configurations['train_params'])\n",
    "testing_set = Dataset(X_test, y_test)\n",
    "testing_generator = torch.utils.data.DataLoader(testing_set, **configurations['test_params'])\n",
    "torch.manual_seed(configurations['random_seed'])\n",
    "\n",
    "# model, loss, optimizer, and loss history\n",
    "net, loss_func, optimizer, mean_losses, all_losses, next_epoch, _, _ = load_model_with_config(configurations)\n",
    "print(net)\n",
    "print(f\"This model has prevoiusly been trained for {next_epoch} of {configurations['max_epochs']} epochs\")\n",
    "\n",
    "# train the network\n",
    "epbar = tqdm(range(next_epoch, configurations['max_epochs']))\n",
    "for epoch in epbar:\n",
    "    net.train()\n",
    "    epbar.set_description(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    elosses = []\n",
    "    vlosses = []\n",
    "\n",
    "    ipbar = tqdm(training_generator, leave=False)\n",
    "    ipbar.set_description(f\"Training\")\n",
    "    \n",
    "    for i, (x, y) in enumerate(ipbar):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(x)     # input x and predict based on x\n",
    "        loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        elosses.append(loss.data.numpy().item())\n",
    "        ipbar.set_postfix({'loss': loss.data.numpy()})\n",
    "\n",
    "    mean_vlosses = 0\n",
    "    if configurations['do_validate']:\n",
    "        net.eval()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            vpbar = tqdm(testing_generator, leave=False)\n",
    "            vpbar.set_description(\"Validating\")\n",
    "            for i, (x, y) in enumerate(vpbar):\n",
    "                prediction = net(x)\n",
    "                loss = loss_func(prediction, y)\n",
    "                vlosses.append(loss.data.numpy().item())\n",
    "                vpbar.set_postfix({'loss': loss.data.numpy()})\n",
    "        mean_vlosses = mean(vlosses)\n",
    "            \n",
    "    mean_elosses = mean(elosses)\n",
    "    mean_losses.append((mean_elosses, mean_vlosses))\n",
    "    all_losses.append((elosses, vlosses))\n",
    "    save_model_with_config(configurations, net=net, loss_func=loss_func, optimizer=optimizer,\n",
    "                           mean_losses=mean_losses, all_losses=all_losses, next_epoch=epoch+1,\n",
    "                           X_min_max_scaler=X_min_max_scaler, y_min_max_scaler=y_min_max_scaler,\n",
    "                          )\n",
    "    \n",
    "    epbar.set_postfix({'train_loss':f\"{mean_elosses:.9f}\", 'val_loss':f\"{mean_vlosses:.9f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl, vl = zip(*mean_losses)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(tl, label=\"Training Loss\")\n",
    "ax.plot(vl, label=\"Validation Loss\")\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b6059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
