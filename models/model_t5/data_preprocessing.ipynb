{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sgp4.api import Satrec, SatrecArray, WGS72\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global dataset\n",
    "dataset = \"test\" # variable for lazy loading defaultdict\n",
    "input_files = [\n",
    "    \"train\",\n",
    "    \"test\",\n",
    "    \"secret_test\",\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "# using defaultdict to lazy load dataframes.... probably should stay in notebook as shortcut only\n",
    "data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}.pkl\"))\n",
    "tle_sup_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../tle_sup/{dataset}.pkl\"))\n",
    "sgp4_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}_sgp4rv.pkl\"))\n",
    "# satrec_data = defaultdict(lambda: pd.read_pickle(f\"{os.environ['GP_HIST_PATH']}/../3_min/{dataset}_satrec.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __jday_convert(x):\n",
    "    '''\n",
    "    Algorithm from python-sgp4:\n",
    "\n",
    "    from sgp4.functions import jday\n",
    "    jday(x.year, x.month, x.day, x.hour, x.minute, x.second + x.microsecond * 1e-6)\n",
    "    '''\n",
    "    jd = (367.0 * x.year\n",
    "         - 7 * (x.year + ((x.month + 9) // 12.0)) * 0.25 // 1.0\n",
    "           + 275 * x.month / 9.0 // 1.0\n",
    "           + x.day\n",
    "         + 1721013.5)\n",
    "    fr = (x.second + (x.microsecond * 1e-6) + x.minute * 60.0 + x.hour * 3600.0) / 86400.0;\n",
    "    return jd, fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_repr(s,v):\n",
    "    cos = np.cos(np.deg2rad(s * (360/v)))\n",
    "    sin = np.sin(np.deg2rad(s * (360/v)))\n",
    "    return cos,sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_values(df):\n",
    "    name = df.name\n",
    "    df = df.sort_values(\"EPOCH\")\n",
    "    \n",
    "    # convert ARG_OF_PERICENTER, RA_OF_ASC_NODE, and MEAN_ANOMALY to non-cyclic version\n",
    "    df[\"ARG_OF_PERICENTER_ADJUSTED\"] = np.cumsum(np.around(df.ARG_OF_PERICENTER.diff().fillna(0) / -360))*360 + df.ARG_OF_PERICENTER\n",
    "    df[\"RA_OF_ASC_NODE_ADJUSTED\"] = np.cumsum(np.around(df.RA_OF_ASC_NODE.diff().fillna(0) / -360))*360 + df.RA_OF_ASC_NODE\n",
    "    \n",
    "    # according to 18 SPCS there was only 1 such case BUT ITS NOT TRUE there are like 70+\n",
    "    # this is because for REV_AT_EPOCH = 100,000, it's recorded as 10,000 instead of 0\n",
    "    # this doesn't handle the case for multiple ground stations reporting though, if the previous is different....\n",
    "    # would it be better to just remove this as an outlier just to be safe?\n",
    "    # 90k +- 20 max offset based on MEAN_MOTION maximum from earlier steps\n",
    "    df.loc[(df.REV_AT_EPOCH==10000) & df.REV_AT_EPOCH.diff().between(-89999,-89940),'REV_AT_EPOCH'] = 0\n",
    "\n",
    "    # combine REV_AT_EPOCH and MEAN_ANOMALY for a non-cyclic representation\n",
    "    adjusted_rev = df.REV_AT_EPOCH + np.cumsum(np.around(df.REV_AT_EPOCH.diff().fillna(0) / -100000)) * 100000\n",
    "    df[\"REV_MEAN_ANOMALY_COMBINED\"] = adjusted_rev * 360 + df.MEAN_ANOMALY\n",
    "    \n",
    "    # this is to handle the REV_AT_EPOCH problem inconsistency problem\n",
    "    # otherwise the REV_MEAN_ANOMALY_COMBINED difference may be incorrect\n",
    "    # bfill because we may start at non-zero due to previous data removal bit\n",
    "    a = np.round((adjusted_rev.diff().fillna(method='bfill')/300)).fillna(0)\n",
    "    df[\"SUBGROUP\"] = np.cumsum(a).astype(int)\n",
    "    \n",
    "    doycos, doysin = cyclic_repr(df.EPOCH.dt.dayofyear, 366)\n",
    "    df[\"DAY_OF_YEAR_COS\"] = doycos\n",
    "    df[\"DAY_OF_YEAR_SIN\"] = doysin\n",
    "    \n",
    "    macos, masin = cyclic_repr(df.MEAN_ANOMALY, 360)\n",
    "    df[\"MEAN_ANOMALY_COS\"] = macos\n",
    "    df[\"MEAN_ANOMALY_SIN\"] = masin\n",
    "    \n",
    "    icos, isin = cyclic_repr(df.INCLINATION, 360)\n",
    "    df[\"INCLINATION_COS\"] = icos\n",
    "    df[\"INCLINATION_SIN\"] = isin\n",
    "    \n",
    "    rcos, rsin = cyclic_repr(df.RA_OF_ASC_NODE, 360)\n",
    "    df[\"RA_OF_ASC_NODE_COS\"] = rcos\n",
    "    df[\"RA_OF_ASC_NODE_SIN\"] = rsin\n",
    "    \n",
    "    df[['EPOCH_JD', 'EPOCH_FR']] = df.EPOCH.apply(__jday_convert).to_list()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is 1 groupby of satellite\n",
    "def generate_X_y(df):\n",
    "    idx = df.name\n",
    "\n",
    "    df = df.reset_index(level=1).drop_duplicates(subset=['EPOCH']).sort_values(\"EPOCH\")\n",
    "    dfs = []\n",
    "    for i in range(0,20):\n",
    "        dfi = pd.concat([df.add_suffix(\"_1\"),df.shift(-i).add_suffix(\"_2\")], axis=1).dropna()\n",
    "        dfs.append(dfi)\n",
    "    ddf = pd.concat(dfs).reset_index(drop=True)\n",
    "    # Reference variables only, DO NOT USE TO TRAIN\n",
    "    __cols = [\n",
    "        'NORAD_CAT_ID_1','GP_ID_1','GP_ID_2','EPOCH_1','EPOCH_2',\n",
    "#         'SAT_RX_2', 'SAT_RY_2', 'SAT_RZ_2', 'SAT_VX_2', 'SAT_VY_2', 'SAT_VZ_2', # these are ground truths\n",
    "    ]\n",
    "    df = ddf[__cols]\n",
    "    df.columns = ['__'+x for x in __cols]\n",
    "    \n",
    "    # X\n",
    "    x_cols = [\n",
    "        'EPOCH_JD_1', 'EPOCH_FR_1', 'EPOCH_JD_2', 'EPOCH_FR_2',\n",
    "        'MEAN_MOTION_DOT_1', 'BSTAR_1', 'INCLINATION_1', 'RA_OF_ASC_NODE_1', 'ECCENTRICITY_1', 'ARG_OF_PERICENTER_1',\n",
    "        'MEAN_ANOMALY_1', 'MEAN_MOTION_1',\n",
    "        'MEAN_ANOMALY_COS_1', 'MEAN_ANOMALY_SIN_1',\n",
    "        'INCLINATION_COS_1', 'INCLINATION_SIN_1',\n",
    "        'RA_OF_ASC_NODE_COS_1', 'RA_OF_ASC_NODE_SIN_1',\n",
    "        'SEMIMAJOR_AXIS_1', 'PERIOD_1', 'APOAPSIS_1', 'PERIAPSIS_1', 'RCS_SIZE_1',\n",
    "        'SAT_RX_1', 'SAT_RY_1', 'SAT_RZ_1', 'SAT_VX_1', 'SAT_VY_1', 'SAT_VZ_1',\n",
    "        'YEAR_1', 'DAY_OF_YEAR_COS_1', 'DAY_OF_YEAR_SIN_1',\n",
    "        'SUNSPOTS_1D_1', 'SUNSPOTS_3D_1', 'SUNSPOTS_7D_1',\n",
    "        'AIR_MONTH_AVG_TEMP_1','WATER_MONTH_AVG_TEMP_1',\n",
    "    ]\n",
    "    \n",
    "    df['X_delta_EPOCH'] = (ddf.EPOCH_2 - ddf.EPOCH_1).astype(int) / 86400000000000 # in days\n",
    "    df[['X_'+x for x in x_cols]] = ddf[x_cols]\n",
    "\n",
    "    y_cols = ['BSTAR', 'INCLINATION', 'RA_OF_ASC_NODE', 'ECCENTRICITY', 'ARG_OF_PERICENTER', 'MEAN_MOTION', 'MEAN_ANOMALY']\n",
    "    df[['y_'+y for y in y_cols]] = ddf[[y+'_2' for y in y_cols]]\n",
    "    \n",
    "    df['y_REV_MA_REG'] = ((ddf.REV_MEAN_ANOMALY_COMBINED_2 - ddf.REV_MEAN_ANOMALY_COMBINED_1) + ddf.MEAN_ANOMALY_1) / 360\n",
    "    df['y_ARG_OF_PERICENTER_REG'] = (ddf.ARG_OF_PERICENTER_ADJUSTED_2 - ddf.ARG_OF_PERICENTER_ADJUSTED_1 + ddf.ARG_OF_PERICENTER_1) / 360\n",
    "    df['y_RA_OF_ASC_NODE_REG'] = (ddf.RA_OF_ASC_NODE_ADJUSTED_2 - ddf.RA_OF_ASC_NODE_ADJUSTED_1 + ddf.RA_OF_ASC_NODE_1) / 360\n",
    "\n",
    "    df = df[(df['X_delta_EPOCH'] < 14) & (df['X_delta_EPOCH'] > 0.04) & ((df.y_REV_MA_REG / df.X_delta_EPOCH).between(df.X_MEAN_MOTION_1*0.99,df.X_MEAN_MOTION_1*1.01))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_the_thing(df, f):\n",
    "    df = df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "    df = df.merge(sgp4_data[f], left_index=True, right_index=True)\n",
    "    df = df.merge(tle_sup_data[f], left_on=\"GP_ID\", right_index=True)\n",
    "    df = df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1247085ec5b247dba522734bd42966b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ef105a5af441e2848e996596b9703a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0852260638f4a688856d5522b0d97c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a6c4a7c47f4983894f0bfcdf58f2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18035161\n",
      "1263171\n"
     ]
    }
   ],
   "source": [
    "# generate smaller set from training set to test\n",
    "prefix = \"sample2_\"\n",
    "\n",
    "dataset = \"train\"\n",
    "train_ids = np.random.choice(data[dataset].NORAD_CAT_ID.unique(), 450)\n",
    "sample_train_df = data[dataset][data[dataset].NORAD_CAT_ID.isin(train_ids)]\n",
    "sample_train_df = do_the_thing(sample_train_df, dataset)\n",
    "sample_train_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t5_data/{prefix}train.pkl\")\n",
    "print(len(sample_train_df))\n",
    "\n",
    "dataset = \"test\"\n",
    "test_ids = np.random.choice(data[dataset].NORAD_CAT_ID.unique(), 50)\n",
    "sample_test_df = data[dataset][data[dataset].NORAD_CAT_ID.isin(test_ids)]\n",
    "sample_test_df = do_the_thing(sample_test_df, dataset)\n",
    "sample_test_df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t5_data/{prefix}test.pkl\")\n",
    "print(len(sample_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788d9641b3394989a3a9be8b32aceba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cac897b08bb46dd9e6988bfb66c5189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06320c154124b5eb6e79d0352a80429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e94d613c0d4328841dee39dd1cb5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for: secret_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f167985b004677842cfa19cebbd45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be37763669f44d7f9db5f77f8d9863e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in input_files:\n",
    "    dataset = f # variable for lazy loading defaultdict\n",
    "    print(f\"Preparing data for: {f}\")\n",
    "    df = do_the_thing(data[f], dataset)\n",
    "    df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t5_data/{f}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset = \"test\" # set the lazy loader\n",
    "# sample_df = data[dataset][data[dataset].NORAD_CAT_ID.isin([20885, 7128])]#, 4756\n",
    "# sample_df = sample_df.groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# # sample_df = sample_df.merge(sgp4_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.merge(tle_sup_data[dataset], left_on=\"GP_ID\", right_index=True)\n",
    "# sample_df = sample_df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# sample_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"train\" # variable for lazy loading defaultdict\n",
    "# print(f\"Preparing data for: {f}\")\n",
    "# df = data[f].groupby(by=\"NORAD_CAT_ID\", as_index=False).progress_apply(convert_feature_values)\n",
    "# df = df.merge(sgp4_data[f], left_index=True, right_index=True)\n",
    "# df = df.merge(tle_sup_data[f], left_on=\"GP_ID\", right_index=True)\n",
    "# df = df.groupby([\"NORAD_CAT_ID\",\"SUBGROUP\"], as_index=False).progress_apply(generate_X_y)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df.to_pickle(f\"{os.environ['GP_HIST_PATH']}/../t5_data/{f}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
